  ("Language model", {"url": "https://en.wikipedia.org/wiki/Language_model", "summary": "A language model is a probability distribution over sequences of words. Given any sequence of words of length m, a language model assigns a probability \n  \n    \n      \n        P\n        (\n        \n          w\n          \n            1\n          \n        \n        ,\n        …\n        ,\n        \n          w\n          \n            m\n          \n        \n        )\n      \n    \n    {\\displaystyle P(w_{1},\\ldots ,w_{m})}\n   to the whole sequence. Language models generate probabilities by training on text corpora in one or many languages. Given that languages can be used to express an infinite variety of valid sentences (the property of digital infinity), language modeling faces the problem of assigning non-zero probabilities to linguistically valid sequences that may never be encountered in the training data. Several modelling approaches have been designed to surmount this problem, such as applying the Markov assumption or using neural architectures such as recurrent neural networks or transformers.\nLanguage models are useful for a variety of problems in computational linguistics; from initial applications in speech recognition to ensure nonsensical (i.e. low-probability) word sequences are not predicted, to wider use in machine translation (e.g. scoring candidate translations), natural language generation (generating more human-like text), part-of-speech tagging, parsing, optical character recognition, handwriting recognition, grammar induction, information retrieval, and other applications.\nLanguage models are used in information retrieval in the query likelihood model. There, a separate language model is associated with each document in a collection. Documents are ranked based on the probability of the query \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n   in the document"s language model \n  \n    \n      \n        \n          M\n          \n            d\n          \n        \n      \n    \n    {\\displaystyle M_{d}}\n  : \n  \n    \n      \n        P\n        (\n        Q\n        ∣\n        \n          M\n          \n            d\n          \n        \n        )\n      \n    \n    {\\displaystyle P(Q\\mid M_{d})}\n  . Commonly, the unigram language model is used for this purpose.\nSince 2018, large language models (LLMs) consisting of deep neural networks with billions of trainable parameters, trained on massive datasets of unlabelled text, have demonstrated impressive results on a wide variety of natural language processing tasks. This development has led to a shift in research focus toward the use of general-purpose LLMs."})
  ("Information retrieval", {"url": "https://en.wikipedia.org/wiki/Information_retrieval", "summary": "Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.\nAutomated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.\n\n"})
  ("Word", {"url": "https://en.wikipedia.org/wiki/Word", "summary": "A word is a basic element of language that carries an objective or practical meaning, can be used on its own, and is uninterruptible. Despite the fact that language speakers often have an intuitive grasp of what a word is, there is no consensus among linguists on its definition and numerous attempts to find specific criteria of the concept remain controversial. Different standards have been proposed, depending on the theoretical background and descriptive context; these do not converge on a single definition.:\u200a13:618\u200a Some specific definitions of the term "word" are employed to convey its different meanings at different levels of description, for example based on phonological, grammatical or orthographic basis. Others suggest that the concept is simply a convention used in everyday situations.:\u200a6\u200aThe concept of "word" is distinguished from that of a morpheme, which is the smallest unit of language that has a meaning, even if it cannot stand on its own. Words are made out of at least one morpheme. Morphemes can also be joined to create other words in a process of morphological derivation.:\u200a768\u200a In English and many other languages, the morphemes that make up a word generally include at least one root (such as "rock", "god", "type", "writ", "can", "not") and possibly some affixes ("-s", "un-", "-ly", "-ness"). Words with more than one root ("[type][writ]er", "[cow][boy]s", "[tele][graph]ically") are called compound words. In turn, words are combined to form other elements of language, such as phrases ("a red rock", "put up with"), clauses ("I threw a rock"), and sentences ("I threw a rock, but missed").\nIn many languages, the notion of what constitutes a "word" may be learned as part of learning the writing system. This is the case for the English language, and for most languages that are written with alphabets derived from the ancient Latin or Greek alphabets. In English orthography, the letter sequences "rock", "god", "write", "with", "the", and "not" are considered to be single-morpheme words, whereas "rocks", "ungodliness", "typewriter", and "cannot" are words composed of two or more morphemes ("rock"+"s", "un"+"god"+"li"+"ness", "type"+"writ"+"er", and "can"+"not")."})
  ("N-gram", {"url": "https://en.wikipedia.org/wiki/N-gram", "summary": "In the field of computational linguistics, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.  The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles.Using Latin numerical prefixes, an n-gram of size 1 is referred to as a "unigram"; size 2 is a "bigram" (or, less commonly, a "digram"); size 3 is a "trigram". English cardinal numbers are sometimes used, e.g., "four-gram", "five-gram", and so on.    In computational biology, a polymer or oligomer of a known size is called a k-mer instead of an n-gram, with specific names using Greek numerical prefixes such as "monomer", "dimer", "trimer", "tetramer", "pentamer", etc., or English cardinal numbers, "one-mer", "two-mer", "three-mer", etc."})
  ("Title", {"url": "https://en.wikipedia.org/wiki/Title", "summary": "A title is one or more words used before or after a person"s name, in certain contexts. It may signify either generation, an official position, or a professional or academic qualification. In some languages, titles may be inserted between the first and last name (for example, Graf in German, Cardinal in Catholic usage (Richard Cardinal Cushing) or clerical titles such as Archbishop). Some titles are hereditary.\n\n"})
  ("Abstract (summary)", {"url": "https://en.wikipedia.org/wiki/Abstract_(summary)", "summary": "An abstract is a brief summary of a research article, thesis, review, conference proceeding, or any in-depth analysis of a particular subject and is often used to help the reader quickly ascertain the paper\"s purpose. When used, an abstract always appears at the beginning of a manuscript or typescript, acting as the point-of-entry for any given academic paper or patent application.  Abstracting and indexing services for various academic disciplines are aimed at compiling a body of literature for that particular subject.\nThe terms précis or synopsis are used in some publications to refer to the same thing that other publications might call an "abstract". In management reports, an executive summary usually contains more information (and often more sensitive information) than the abstract does."})
  ("Likelihood function", {"url": "https://en.wikipedia.org/wiki/Likelihood_function", "summary": "The likelihood function (often simply called the likelihood) is the joint probability of the observed data viewed as a function of the parameters of a statistical model.In maximum likelihood estimation, the arg max (over \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  ) of the likelihood function serves as a point estimate for \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  , while the Fisher information (often approximated by the likelihood"s Hessian matrix) indicates the estimate"s precision. Meanwhile in Bayesian statistics, parameter estimates are derived from the converse of the likelihood, the so-called posterior probability, which is calculated via Bayes" rule."})
  ("Probability", {"url": "https://en.wikipedia.org/wiki/Probability", "summary": "In science, the probability of an event is a number that indicates how likely the event is to occur. It is expressed as a number in the range from 0 and 1, or, using percentage notation, in the range from 0% to 100%.  The more likely it is that the event will occur, the higher its probability. The probability of an impossible event is 0; that of an event that is certain to occur is 1. The probabilities of two complementary events A and B – either A occurs or B occurs – add up to 1. A simple example is the tossing of a fair (unbiased) coin. If a coin is fair, the two possible outcomes ("heads" and "tails") are equally likely; since these two outcomes are complementary and the probability of "heads" equals the probability of "tails", the probability of each of the two outcomes equals 1/2 (which could also be written as 0.5 or 50%).\nThese concepts have been given an axiomatic mathematical formalization in probability theory, a branch of mathematics that is used in areas of study such as statistics, mathematics, science, finance, gambling, artificial intelligence, machine learning, computer science and game theory to, for example, draw inferences about the expected frequency of events. Probability theory is also used to describe the underlying mechanics and regularities of complex systems."})
  ("Bigram", {"url": "https://en.wikipedia.org/wiki/Bigram", "summary": "A bigram or digram is a sequence of two adjacent elements from a string of tokens, which are typically letters, syllables, or words. A bigram is an n-gram for n=2.\nThe frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, and speech recognition.\nGappy bigrams or skipping bigrams are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a dependency grammar).\n\n"})
  ("Bayes' theorem", {"url": "https://en.wikipedia.org/wiki/Bayes%27_theorem", "summary": "In probability theory and statistics, Bayes\" theorem (alternatively Bayes\" law or Bayes\" rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes\" theorem allows the risk to an individual of a known age to be assessed more accurately by conditioning it relative to their age, rather than simply assuming that the individual is typical of the population as a whole.\nOne of the many applications of Bayes\" theorem is Bayesian inference, a particular approach to statistical inference. When applied, the probabilities involved in the theorem may have different probability interpretations. With Bayesian probability interpretation, the theorem expresses how a degree of belief, expressed as a probability, should rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics, being considered by one authority as; "to the theory of probability what Pythagoras\"s theorem is to geometry.""})
  ("Conditional probability", {"url": "https://en.wikipedia.org/wiki/Conditional_probability", "summary": "In probability theory, conditional probability is a measure of the probability of an event occurring, given that another event (by assumption, presumption, assertion or evidence) has already occurred. This particular method relies on event B occurring with some sort of relationship with another event A. In this event, the event B can be analyzed by a conditional probability with respect to A. If the event of interest is A and the event B is known or assumed to have occurred, "the conditional probability of A given B", or "the probability of A under the condition B", is usually written as P(A|B) or occasionally PB(A). This can also be understood as the fraction of probability B that intersects with A: \n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              A\n              ∩\n              B\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}}\n  .For example, the probability that any given person has a cough on any given day may be only 5%. But if we know or assume that the person is sick, then they are much more likely to be coughing. For example, the conditional probability that someone unwell (sick) is coughing might be 75%, in which case we would have that P(Cough) = 5% and P(Cough|Sick) = 75 %. Although there is a relationship between A and B in this example, such a relationship or dependence between A and B is not necessary, nor do they have to occur simultaneously.\nP(A|B) may or may not be equal to P(A) (the unconditional probability of A). If P(A|B) = P(A), then events A and B are said to be independent: in such a case, knowledge about either event does not alter the likelihood of each other. P(A|B) (the conditional probability of A given B) typically differs from P(B|A). For example, if a person has dengue fever, the person might have a 90% chance of being tested as positive for the disease. In this case, what is being measured is that if event B (having dengue) has occurred, the probability of A (tested as positive) given that B occurred is 90%, simply writing P(A|B) = 90%. Alternatively, if a person is tested as positive for dengue fever, they may have only a 15% chance of actually having this rare disease due to high false positive rates. In this case, the probability of the event B (having dengue) given that the event A (testing positive) has occurred is 15% or P(B|A) = 15%. It should be apparent now that falsely equating the two probabilities can lead to various errors of reasoning, which is commonly seen through base rate fallacies.  \nWhile conditional probabilities can provide extremely useful information, limited information is often supplied or at hand. Therefore, it can be useful to reverse or convert a conditional probability using Bayes\" theorem: \n  \n    \n      \n        P\n        (\n        A\n        ∣\n        B\n        )\n        =\n        \n          \n            \n              P\n              (\n              B\n              ∣\n              A\n              )\n              P\n              (\n              A\n              )\n            \n            \n              P\n              (\n              B\n              )\n            \n          \n        \n      \n    \n    {\\displaystyle P(A\\mid B)={{P(B\\mid A)P(A)} \\over {P(B)}}}\n  .  Another option is to display conditional probabilities in conditional probability table to illuminate the relationship between events.\n\n"})
  ("Maximum likelihood estimation", {"url": "https://en.wikipedia.org/wiki/Maximum_likelihood_estimation", "summary": "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for finding maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved analytically; for instance, the ordinary least squares estimator for a linear regression model maximizes the likelihood when all observed outcomes are assumed to have normal distributions with the same variance.From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation with uniform prior distributions (or a normal prior distribution with a standard deviation of infinity). In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.\n\n"})
  ("Estimation", {"url": "https://en.wikipedia.org/wiki/Estimation", "summary": "Estimation (or estimating) is the process of finding an estimate or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable. The value is nonetheless usable because it is derived from the best information available. Typically, estimation involves "using the value of a statistic derived from a sample to estimate the value of a corresponding population parameter". The sample provides information that can be projected, through various formal or informal processes, to determine a range most likely to describe the missing information. An estimate that turns out to be incorrect will be an overestimate if the estimate exceeds the actual result and an underestimate if the estimate falls short of the actual result."})
  ("Speech recognition", {"url": "https://en.wikipedia.org/wiki/Speech_recognition", "summary": "Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.\nSome speech recognition systems require "training" (also called "enrollment") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person\"s specific voice and uses it to fine-tune the recognition of that person\"s speech, resulting in increased accuracy. Systems that do not use training are called "speaker-independent" systems. Systems that use training are called "speaker dependent".\nSpeech recognition applications include voice user interfaces such as voice dialing (e.g. "call home"), call routing (e.g. "I would like to make a collect call"), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).\nThe term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person\"s voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.\nFrom the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems."})
  ("Training, validation, and test data sets", {"url": "https://en.wikipedia.org/wiki/Training,_validation,_and_test_data_sets", "summary": "In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided into multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation, and test sets.\nThe model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.\nSuccessively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model\"s hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).\nThis simple procedure is complicated in practice by the fact that the validation dataset\"s error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term "validation set" is sometimes used instead of "test set" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available."})
  ("Prior probability", {"url": "https://en.wikipedia.org/wiki/Prior_probability", "summary": "A prior probability distribution of an uncertain quantity, often simply called the prior, is its assumed probability distribution before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable.\nIn Bayesian statistics, Bayes" rule prescribes how to update the prior with new information to obtain the posterior probability distribution, which is the conditional distribution of the uncertain quantity given new data. Historically, the choice of priors was often constrained to a conjugate family of a given likelihood function, for that it would result in a tractable posterior of the same family. The widespread availability of Markov chain Monte Carlo methods, however, has made this less of a concern.\nThere are many ways to construct a prior distribution. In some cases, a prior may be determined from past information, such as previous experiments. A prior can also be elicited from the purely subjective assessment of an experienced expert. When no information is available, an uninformative prior may be adopted as justified by the principle of indifference. In modern applications, priors are also often chosen for their mechanical properties, such as regularization and feature selection.The prior distributions of model parameters will often depend on parameters of their own. Uncertainty about these hyperparameters can, in turn, be expressed as hyperprior probability distributions. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:\n\np is a parameter of the underlying system (Bernoulli distribution), and\nα and β are parameters of the prior distribution (beta distribution); hence hyperparameters.In principle, priors can be decomposed into many conditional levels of distributions, so-called hierarchical priors."})
  ("Probability distribution", {"url": "https://en.wikipedia.org/wiki/Probability_distribution", "summary": "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss ("the experiment"), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). Examples of random phenomena include the weather conditions at some future date, the height of a randomly selected person, the fraction of male students in a school, the results of a survey to be conducted, etc."})
  ("Bayesian inference", {"url": "https://en.wikipedia.org/wiki/Bayesian_inference", "summary": "Bayesian inference is a method of statistical inference in which Bayes\" theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called "Bayesian probability"."})
  ("Inverted index", {"url": "https://en.wikipedia.org/wiki/Inverted_index", "summary": "In computer science, an inverted index (also referred to as a postings list, postings file, or inverted file) is a database index storing a mapping from content, such as words or numbers, to its locations in a table, or in a document or a set of documents (named in contrast to a forward index, which maps from documents to content). The purpose of an inverted index is to allow fast full-text searches, at a cost of increased processing when a document is added to the database. The inverted file may be the database file itself, rather than its index. It is the most popular data structure used in document retrieval systems, used on a large scale for example in search engines. Additionally, several significant general-purpose mainframe-based database management systems have used inverted list architectures, including ADABAS, DATACOM/DB, and Model 204.\nThere are two main variants of inverted indexes: A record-level inverted index (or inverted file index or just inverted file) contains a list of references to documents for each word. A word-level inverted index (or full inverted index or inverted list) additionally contains the positions of each word within a document. The latter form offers more functionality (like phrase searches), but needs more processing power and space to be created."})
  ("Data structure", {"url": "https://en.wikipedia.org/wiki/Data_structure", "summary": "In computer science, a data structure is a data organization, management, and storage format that is usually chosen for efficient access to data. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.\n\n"})
  ("Okapi BM25", {"url": "https://en.wikipedia.org/wiki/Okapi_BM25", "summary": "In information retrieval, Okapi BM25 (BM is an abbreviation of best matching) is a ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on the probabilistic retrieval framework developed in the 1970s and 1980s by Stephen E. Robertson, Karen Spärck Jones, and others.\nThe name of the actual ranking function is BM25. The fuller name, Okapi BM25, includes the name of the first system to use it, which was the Okapi information retrieval system, implemented at London"s City University in the 1980s and 1990s. BM25 and its newer variants, e.g. BM25F (a version of BM25 that can take document structure and anchor text into account), represent TF-IDF-like retrieval functions used in document retrieval.\n\n"})
  ("Ranking (information retrieval)", {"url": "https://en.wikipedia.org/wiki/Ranking_(information_retrieval)", "summary": "Ranking of query is one of the fundamental problems in information retrieval (IR), the scientific/engineering discipline behind search engines. Given a query q and a collection D of documents that match the query, the problem is to rank, that is, sort, the documents in D according to some criterion so that the "best" results appear early in the result list displayed to the user. Ranking in terms of information retrieval is an important concept in computer science and is used in many different applications such as search engine queries and recommender systems. A majority of search engines use ranking algorithms to provide users with accurate and relevant results."})
  ("Neurotransmitter", {"url": "https://en.wikipedia.org/wiki/Neurotransmitter", "summary": "A neurotransmitter is a signaling molecule secreted by a neuron to affect another cell across a synapse. The cell receiving the signal, or target cell, may be another neuron, but could also be a gland or muscle cell.Neurotransmitters are released from synaptic vesicles into the synaptic cleft where they are able to interact with neurotransmitter receptors on the target cell. The neurotransmitter"s effect on the target cell is determined by the receptor it binds to. Many neurotransmitters are synthesized from simple and plentiful precursors such as amino acids, which are readily available and often require a small number of biosynthetic steps for conversion.\nNeurotransmitters are essential to the function of complex neural systems. The exact number of unique neurotransmitters in humans is unknown, but more than 100 have been identified. Common neurotransmitters include glutamate, GABA, acetylcholine, glycine and norepinephrine."})
  ("Paragraph", {"url": "https://en.wikipedia.org/wiki/Paragraph", "summary": "A paragraph (from Ancient Greek  παράγραφος (parágraphos) "to write beside") is a self-contained unit of discourse in writing dealing with a particular point or idea. Though not required by the orthographic conventions of any language with a writing system, paragraphs are a conventional means of organizing extended segments of prose.\n\n"})
  ("Document", {"url": "https://en.wikipedia.org/wiki/Document", "summary": "A document is a written, drawn, presented, or memorialized representation of thought, often the manifestation of non-fictional, as well as fictional, content.  The word originates from the Latin Documentumcode: lat promoted to code: la , which denotes a "teaching" or "lesson": the verb doceōcode: lat promoted to code: la  denotes "to teach". In the past, the word was usually used to denote written proof useful as evidence of a truth or fact. In the Computer Age, "document" usually denotes a primarily textual computer file, including its structure and format, e.g. fonts, colors, and images. Contemporarily, "document" is not defined by its transmission medium, e.g., paper, given the existence of electronic documents. "Documentation" is distinct because it has more denotations than "document". Documents are also distinguished from "realia", which are three-dimensional objects that would otherwise satisfy the definition of "document" because they memorialize or represent thought; documents are considered more as 2-dimensional representations. While documents can have large varieties of customization, all documents can be shared freely and have the right to do so, creativity can be represented by documents, also. History, events, examples, opinions, etc. all can be expressed in documents."})
  ("Cosine similarity", {"url": "https://en.wikipedia.org/wiki/Cosine_similarity", "summary": "In data analysis, cosine similarity is a measure of similarity between two non-zero vectors defined in an inner product space. Cosine similarity is the cosine of the angle between the vectors; that is, it is the dot product of the vectors divided by the product of their lengths. It follows that the cosine similarity does not depend on the magnitudes of the vectors, but only on their angle. The cosine similarity always belongs to the interval \n  \n    \n      \n        [\n        −\n        1\n        ,\n        1\n        ]\n        .\n      \n    \n    {\\displaystyle [-1,1].}\n   For example, two proportional vectors have a cosine similarity of 1, two orthogonal vectors have a similarity of 0, and two opposite vectors have a similarity of -1. In some contexts, the component values of the vectors cannot be negative, in which case the cosine similarity is bounded in \n  \n    \n      \n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle [0,1]}\n  . \nFor example, in information retrieval and text mining, each word is assigned a different coordinate and a document is represented by the vector of the numbers of occurrences of each word in the document. Cosine similarity then gives a useful measure of how similar two documents are likely to be, in terms of their subject matter, and independently of the length of the documents.The technique is also used to measure cohesion within clusters in the field of data mining.One advantage of cosine similarity is its low complexity, especially for sparse vectors: only the non-zero coordinates need to be considered.\nOther names for cosine similarity include Orchini similarity and Tucker coefficient of congruence; the Otsuka–Ochiai similarity (see below) is cosine similarity applied to binary data.\n\n"})
  ("Similarity score", {"url": "https://en.wikipedia.org/wiki/Similarity_score", "summary": "In sabermetrics and basketball analytics, similarity scores are a method of comparing baseball and basketball players (usually in MLB or the NBA) to other players, with the intent of discovering who the most similar historical players are to a certain player.\nSimilarity scores are among the many original sabermetric concepts first introduced by Bill James. James initially created the concept as a way to effectively compare non-Hall of Fame players to players in the Hall, to see who was either on track to make the HOF, or to determine if any eligible players had been snubbed by the selection committee. For example, if the most similar players to a non-HOFer were all in the Hall of Fame, one could effectively argue that that player should be in the Hall.\nMore recently, similarity scores have been used to determine career paths and projected statistics for players. The logic behind this line of thought is simple: players often follow similar career trajectories to their most similar players, so the historical similar players\" performance in years after the active player\"s current age should be a good predictor of that active player\"s future production. An example of this would be the Football Outsiders\" discovery that all but the highest caliber of wide receivers suffer a marked decline after their seventh season in the NFL, a fact that bore out for the receivers selected in the 1996 NFL Draft when their production collectively slipped.Many baseball analysts have augmented James\" method over the years, or come up with their own system of measuring similarity. Baseball Prospectus employs a projection system developed by Nate Silver known as PECOTA which applies nearest neighbor analysis to calculate similarities between players from different eras.  Pro Football Prospectus (written by Football Outsiders) has their own system (dubbed "KUBIAK" after longtime Broncos backup quarterback Gary Kubiak) for projecting future performance. John Hollinger developed a similar system for basketball players in his Pro Basketball Forecast series of books, and several APBRmetricians have expanded on his methodology. Similarity scores are also used extensively in many statistical forecasting programs."})
  ("Receiver operating characteristic", {"url": "https://en.wikipedia.org/wiki/Receiver_operating_characteristic", "summary": "A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.  The method was originally developed for operators of military radar receivers starting in 1941, which led to its name.\nThe ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). The ROC can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from \n  \n    \n      \n        −\n        ∞\n      \n    \n    {\\displaystyle -\\infty }\n   to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.\nROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.\nThe ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research.\nThe ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes."})
  ("Relevance", {"url": "https://en.wikipedia.org/wiki/Relevance", "summary": "Relevance is the concept of one topic being connected to another topic in a way that makes it useful   to consider the second topic when considering the first. The concept of relevance is studied in many different fields, including cognitive sciences, logic, and library and information science. Most fundamentally, however, it is studied in epistemology (the theory of knowledge). Different theories of knowledge have different implications for what is considered relevant and these fundamental views have implications for all other fields as well."})
  ("Tf–idf", {"url": "https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "summary": "In information retrieval, tf–idf (also TF*IDF, TFIDF, TF–IDF, or Tf–idf), short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\nThe tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf–idf has been one of the most popular term-weighting schemes. A survey conducted in 2015 showed that 83% of text-based recommender systems in digital libraries use tf–idf.Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document"s relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\nOne of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model."})
  ("1995", {"url": "https://en.wikipedia.org/wiki/1995", "summary": "1995 (MCMXCV) was a common year starting on Sunday of the Gregorian calendar, the 1995th year of the Common Era (CE) and Anno Domini (AD) designations, the 995th  year of the 2nd millennium, the 95th  year of the 20th century, and the  6th   year of the 1990s decade.  \n1995 was designated as:\n\nUnited Nations Year for Tolerance\nWorld Year of Peoples" Commemoration of the Victims of the Second World WarThis was the first year that the Internet was entirely privatized, with the United States government no longer providing public funding, marking the beginning of the Information Age. America Online and Prodigy offered access to the World Wide Web system for the first time this year, releasing browsers that made it easily accessible to the general public."})
  ("Journal of Documentation", {"url": "https://en.wikipedia.org/wiki/Journal_of_Documentation", "summary": "The Journal of Documentation is a double-blind peer-reviewed academic journal covering theories, concepts, models, frameworks, and philosophies in information science. The journal publishes scholarly articles, research reports, and critical reviews.\nThe scope of the Journal of Documentation is broadly information sciences, encompassing all of the academic and professional disciplines which deal with recorded information. These include, but are not limited to, information science, library science, and related disciplines, knowledge management, knowledge organization, information seeking, information retrieval, human information behaviour, and digital literacy.The main audience for the journal is educators, scholars, researchers, and policy-makers in information-related areas. It published quarterly between 1945 and 1996, expanding to five issues per year between 1997 and 1999. Since 2000, it is published bimonthly. It is currently edited by David Bawden (City University London.) In celebration of the journal"s 60th anniversary, a series of review articles were published between 2004 and 2006, which commented on the significance of eleven articles originally published in the Journal in the previous six decades.The journal is published by Emerald Group Publishing. It is available for subscription singularly and also as part of an online subscription to the Emerald Library and Information Studies Subject Collection."})
  ("Cambridge University Press", {"url": "https://en.wikipedia.org/wiki/Cambridge_University_Press", "summary": "Cambridge University Press is the university press of the University of Cambridge. Granted letters patent by King Henry VIII in 1534, it is the oldest university press in the world. It is also the King"s Printer.Cambridge University Press is a department of the University of Cambridge and is both an academic and educational publisher. It became part of Cambridge University Press & Assessment, following a merger with Cambridge Assessment in 2021. With a global sales presence, publishing hubs, and offices in more than 40 countries, it publishes over 50,000 titles by authors from over 100 countries. Its publishing includes more than 380 academic journals, monographs, reference works, school and university textbooks, and English language teaching and learning publications. It also publishes Bibles, runs a bookshop in Cambridge, sells through Amazon, and has a conference venues business in Cambridge at the Pitt Building and the Sir Geoffrey Cass Sports and Social Centre.\nBeing part of the University of Cambridge gives Cambridge University Press a non-profit status. It transfers a minimum of 30% of any annual surplus back to the University of Cambridge.\n\n"})
  ("Information seeking", {"url": "https://en.wikipedia.org/wiki/Information_seeking", "summary": "Information seeking is the process or activity of attempting to obtain information in both human and technological contexts. Information seeking is related to, but different from, information retrieval (IR)."})
  ("Problem solving", {"url": "https://en.wikipedia.org/wiki/Problem_solving", "summary": "Problem solving is the process of achieving a goal by overcoming obstacles, a frequent part of most activities. Problems in need of solutions range from simple personal tasks (e.g. how to turn on an appliance) to complex issues in business and technical fields. The former is an example of simple problem solving (SPS) addressing one issue, whereas the latter is complex problem solving (CPS) with multiple interrelated obstacles. Another classification is into well-defined problems with specific obstacles and goals, and ill-defined problems in which the current situation is troublesome but it is not clear what kind of resolution to aim for. Similarly, one may distinguish formal or fact-based problems requiring psychometric intelligence, versus socio-emotional problems which depend on the changeable emotions of individuals or groups, such as tactful behavior, fashion, or gift choices.Solutions require sufficient resources and knowledge to attain the goal. Professionals such as lawyers, doctors, and consultants are largely problem solvers for issues which require technical skills and knowledge beyond general competence. Many businesses have found profitable markets by recognizing a problem and creating a solution: the more widespread and inconvenient the problem, the greater the opportunity to develop a scalable solution.\nThere are many specialized problem-solving techniques and methods in fields such as engineering, business, medicine, mathematics, computer science, philosophy, and social organization. The mental techniques to identify, analyze, and solve problems are studied in psychology and cognitive sciences. Additionally, the mental obstacles preventing people from finding solutions is a widely researched topic: problem solving impediments include confirmation bias, mental set, and functional fixedness."})
  ("Relevance feedback", {"url": "https://en.wikipedia.org/wiki/Relevance_feedback", "summary": "Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or "pseudo" feedback."})
  ("Cognition", {"url": "https://en.wikipedia.org/wiki/Cognition", "summary": "Cognition refers to "the mental action or process of acquiring knowledge and understanding through thought, experience, and the senses". It encompasses all aspects of intellectual functions and processes such as: perception, attention, thought, imagination, intelligence, the formation of knowledge, memory and working memory, judgment and evaluation, reasoning and computation, problem solving and decision making, comprehension and production of language. Cognitive processes use existing knowledge and discover new knowledge.\nCognitive processes are analyzed from different perspectives within different contexts, notably in the fields of linguistics, musicology, anesthesia, neuroscience, psychiatry, psychology, education, philosophy, anthropology, biology, systemics, logic, and computer science. These and other approaches to the analysis of cognition (such as embodied cognition) are synthesized in the developing field of cognitive science, a progressively autonomous academic discipline."})
  ("Affect (psychology)", {"url": "https://en.wikipedia.org/wiki/Affect_(psychology)", "summary": "Affect, in psychology, refers to the underlying experience of feeling, emotion, attachment, or mood."})
  ("Uncertainty", {"url": "https://en.wikipedia.org/wiki/Uncertainty", "summary": "Uncertainty refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable or stochastic environments, as well as due to ignorance, indolence, or both. It arises in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, medicine, psychology, sociology, engineering, metrology, meteorology, ecology and information science."})
  ("Signs and symptoms", {"url": "https://en.wikipedia.org/wiki/Signs_and_symptoms", "summary": "Signs and symptoms are the observed or detectable signs, and experienced symptoms of an illness,  injury, or condition. Signs are objective and externally observable; symptoms are a person"s reported subjective experiences. A sign for example may be a higher or lower temperature than normal, raised or lowered blood pressure or an abnormality showing on a medical scan. A symptom is something out of the ordinary that is experienced by an individual such as feeling feverish, a headache or other pain or pains in the body."})
  ("Confusion", {"url": "https://en.wikipedia.org/wiki/Confusion", "summary": "In medicine, confusion is the quality or state of being bewildered or unclear. The term "acute mental confusion" is often used interchangeably with delirium in the International Statistical Classification of Diseases and Related Health Problems and the Medical Subject Headings publications to describe the pathology. These refer to the loss of orientation, or the ability to place oneself correctly in the world by time, location and personal identity. Mental confusion is sometimes accompanied by disordered consciousness (the loss of linear thinking) and memory loss (the inability to correctly recall previous events or learn new material)."})
  ("Frustration", {"url": "https://en.wikipedia.org/wiki/Frustration", "summary": "In psychology, frustration is a common emotional response to opposition, related to anger, annoyance and disappointment. Frustration arises from the perceived resistance to the fulfillment of an individual"s will or goal and is likely to increase when a will or goal is denied or blocked. There are two types of frustration: internal and external. Internal frustration may arise from challenges in fulfilling personal goals, desires, instinctual drives and needs, or dealing with perceived deficiencies, such as a lack of confidence or fear of social situations. Conflict, such as when one has competing goals that interfere with one another, can also be an internal source of frustration or annoyance and can create cognitive dissonance. External causes of frustration involve conditions outside an individual"s control, such as a physical roadblock, a difficult task, or the perception of wasting time. There are multiple ways individuals cope with frustration such as passive–aggressive behavior, anger, or violence, although frustration may also propel positive processes via enhanced effort and strive. This broad range of potential outcomes makes it difficult to identify the original cause(s) of frustration, as the responses may be indirect. However, a more direct and common response is a propensity towards aggression."})
  ("Hypertext", {"url": "https://en.wikipedia.org/wiki/Hypertext", "summary": "Hypertext is text displayed on a computer display or other electronic devices with references (hyperlinks) to other text that the reader can immediately access. Hypertext documents are interconnected by hyperlinks, which are typically activated by a mouse click, keypress set, or screen touch. Apart from text, the term "hypertext" is also sometimes used to describe tables, images, and other presentational content formats with integrated hyperlinks. Hypertext is one of the key underlying concepts of the World Wide Web, where Web pages are often written in the Hypertext Markup Language (HTML). As implemented on the Web, hypertext enables the easy-to-use publication of information over the Internet.\n\n"})
  ("Statistical model", {"url": "https://en.wikipedia.org/wiki/Statistical_model", "summary": "A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is "a formal representation of a theory" (Herman Adèr quoting Kenneth Bollen).All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.\n\n"})
  ("Vector space", {"url": "https://en.wikipedia.org/wiki/Vector_space", "summary": "In mathematics and physics, a vector space (also called a linear space) is a set whose elements, often called vectors, may be added together and multiplied ("scaled") by numbers called scalars. Scalars are often real numbers, but can be  complex numbers or, more generally, elements of any field. The operations of vector addition and scalar multiplication must satisfy certain requirements, called vector axioms. The terms real vector space and complex vector space are often used to specify the nature of the scalars: real coordinate space or complex coordinate space.\nVector spaces generalize Euclidean vectors, which allow modeling of physical quantities, such as forces and velocity, that have not only a magnitude, but also a direction. The concept of vector spaces is fundamental for linear algebra, together with the concept of matrix, which allows computing in vector spaces. This provides a concise and synthetic way for manipulating and studying systems of linear equations.\nVector spaces are characterized by their dimension, which, roughly speaking, specifies the number of independent directions in the space. This means that, for two vector spaces over a given field and with the same dimension, the properties that depend only on the vector-space structure are exactly the same (technically the vector spaces are isomorphic). A vector space is finite-dimensional if its dimension is a natural number. Otherwise, it is infinite-dimensional, and its dimension is an infinite cardinal. Finite-dimensional vector spaces occur naturally in geometry and related areas. Infinite-dimensional vector spaces occur in many areas of mathematics. For example, polynomial rings are countably infinite-dimensional vector spaces, and many function spaces have the cardinality of the continuum as a dimension.\nMany vector spaces that are considered in mathematics are also endowed with other structures. This is the case of algebras, which include field extensions, polynomial rings, associative algebras and Lie algebras. This is also the case of topological vector spaces, which include function spaces, inner product spaces, normed spaces, Hilbert spaces and Banach spaces."})
  ("Richard Marcus", {"url": "https://en.wikipedia.org/wiki/Richard_Marcus", "summary": "Richard Marcus (born September 19, 1945) is an American actor who is best known for his roles in St. Elsewhere, Tremors, and The Pretender."})
  ("Sensitivity and specificity", {"url": "https://en.wikipedia.org/wiki/Sensitivity_and_specificity", "summary": "Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. If individuals who have the condition are considered "positive" and those who don\"t are considered "negative", then sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives:\n\nSensitivity (true positive rate) is the probability of a positive test result, conditioned on the individual truly being positive.\nSpecificity (true negative rate) is the probability of a negative test result, conditioned on the individual truly being negative.If the true status of the condition cannot be known, sensitivity and specificity can be defined relative to a "gold standard test" which is assumed correct. For all testing, both diagnostic and screening, there is usually a trade-off between sensitivity and specificity, such that higher sensitivities will mean lower specificities and vice versa.\nA test which reliably detects the presence of a condition, resulting in a high number of true positives and low number of false negatives, will have a high sensitivity. This is especially important when the consequence of failing to treat the condition is serious and/or the treatment is very effective and has minimal side effects.\nA test which reliably excludes individuals who do not have the condition, resulting in a high number of true negatives and low number of false positives, will have a high specificity. This is especially important when people who are identified as having a condition may be subjected to more testing, expense, stigma, anxiety, etc.\n\nThe terms "sensitivity" and "specificity" were introduced by American biostatistician Jacob Yerushalmy in 1947."})
  ("Positive and negative predictive values", {"url": "https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values", "summary": "The positive and negative predictive values (PPV and NPV respectively) are the proportions of positive and negative results in statistics and diagnostic tests that are true positive and true negative results, respectively.  The PPV and NPV describe the performance of a diagnostic test or other statistical measure. A high result can be interpreted as indicating the accuracy of such a statistic. The PPV and NPV are not intrinsic to the test (as true positive rate and true negative rate are); they depend also on the prevalence. Both PPV and NPV can be derived using Bayes" theorem.\nAlthough sometimes used synonymously, a positive predictive value generally refers to what is established by control groups, while a post-test probability refers to a probability for an individual. Still, if the individual"s pre-test probability of the target condition is the same as the prevalence in the control group used to establish the positive predictive value, the two are numerically equal.\nIn information retrieval, the PPV statistic is often called the precision."})
  ("Computer", {"url": "https://en.wikipedia.org/wiki/Computer", "summary": "A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations (computation) automatically. Modern digital electronic computers can perform generic sets of operations known as programs. These programs enable computers to perform a wide range of tasks. A computer system is a nominally complete computer that includes the hardware, operating system (main software), and peripheral equipment needed and used for full operation. This term may also refer to a group of computers that are linked and function together, such as a computer network or computer cluster.\nA broad range of industrial and consumer products use computers as control systems. Simple special-purpose devices like microwave ovens and remote controls are included, as are factory devices like industrial robots and computer-aided design, as well as general-purpose devices like personal computers and mobile devices like smartphones. Computers power the Internet, which links billions of other computers and users.\nEarly computers were meant to be used only for calculations. Simple manual instruments like the abacus have aided people in doing calculations since ancient times. Early in the Industrial Revolution, some mechanical devices were built to automate long, tedious tasks, such as guiding patterns for looms. More sophisticated electrical machines did specialized analog calculations in the early 20th century. The first digital electronic calculating machines were developed during World War II. The first semiconductor transistors in the late 1940s were followed by the silicon-based MOSFET (MOS transistor) and monolithic integrated circuit  chip technologies in the late 1950s, leading to the microprocessor and the microcomputer revolution in the 1970s. The speed, power and versatility of computers have been increasing dramatically ever since then, with transistor counts increasing at a rapid pace (as predicted by Moore"s law), leading to the Digital Revolution during the late 20th to early 21st centuries.\nConventionally, a modern computer consists of at least one processing element, typically a central processing unit (CPU) in the form of a microprocessor, along with some type of computer memory, typically semiconductor memory chips. The processing element carries out arithmetic and logical operations, and a sequencing and control unit can change the order of operations in response to stored information. Peripheral devices include input devices (keyboards, mice, joystick, etc.), output devices (monitor screens, printers, etc.), and input/output devices that perform both functions (e.g., the 2000s-era touchscreen). Peripheral devices allow information to be retrieved from an external source and they enable the result of operations to be saved and retrieved."})
  ("Computing", {"url": "https://en.wikipedia.org/wiki/Computing", "summary": "Computing is any goal-oriented activity requiring, benefiting from, or creating computing machinery. It includes the study and experimentation of algorithmic processes, and development of both hardware and software. Computing has scientific, engineering, mathematical, technological and social aspects. Major computing disciplines include computer engineering, computer science, cybersecurity, data science, information systems, information technology and software engineering.The term computing is also synonymous with counting and calculating. In earlier times, it was used in reference to the action performed by mechanical computing machines, and before that, to human computers."})
  ("Lp space", {"url": "https://en.wikipedia.org/wiki/Lp_space", "summary": "In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). \nLp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, economics, finance, engineering, and other disciplines."})
  ("Fox", {"url": "https://en.wikipedia.org/wiki/Fox", "summary": "Foxes are small to medium-sized, omnivorous mammals belonging to several genera of the family Canidae. They have a flattened skull, upright, triangular ears, a pointed, slightly upturned snout, and a long bushy tail ("brush").\nTwelve species belong to the monophyletic "true fox" group of genus Vulpes. Approximately another 25 current or extinct species are always or sometimes called foxes; these foxes are either part of the paraphyletic group of the South American foxes, or of the outlying group, which consists of the bat-eared fox, gray fox, and island fox.Foxes live on every continent except Antarctica. The most common and widespread species of fox is the red fox (Vulpes vulpes) with about 47 recognized subspecies. The global distribution of foxes, together with their widespread reputation for cunning, has contributed to their prominence in popular culture and folklore in many societies around the world. The hunting of foxes with packs of hounds, long an established pursuit in Europe, especially in the British Isles, was exported by European settlers to various parts of the New World."})
  ("Natural language", {"url": "https://en.wikipedia.org/wiki/Natural_language", "summary": "In neuropsychology, linguistics, and philosophy of language, a natural language or ordinary language is any language that has evolved naturally in humans through use and repetition without conscious planning or premeditation. Natural languages can take different forms, such as speech or signing. They are distinguished from constructed and formal languages such as those used to program computers or to study logic."})
  ("Query language", {"url": "https://en.wikipedia.org/wiki/Query_language", "summary": "A query language, also known as data query language or database query language (DQL), is a computer language used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL)."})
  ("Information space", {"url": "https://en.wikipedia.org/wiki/Information_space", "summary": "Information space is the set of concepts, and relations among them, held by an information system; it describes the range of possible values or meanings an entity can have under the given rules and circumstances."})
  ("Latent semantic analysis", {"url": "https://en.wikipedia.org/wiki/Latent_semantic_analysis", "summary": "Latent semantic analysis (LSA) is a technique in natural language processing, in particular distributional semantics, of analyzing relationships between a set of documents and the terms they contain by producing a set of concepts related to the documents and terms.  LSA assumes that words that are close in meaning will occur in similar pieces of text (the distributional hypothesis).  A matrix containing word counts per document (rows represent unique words and columns represent each document) is constructed from a large piece of text and a mathematical technique called singular value decomposition (SVD) is used to reduce the number of rows while preserving the similarity structure among columns.  Documents are then compared by cosine similarity between any two columns.  Values close to 1 represent very similar documents while values close to 0 represent very dissimilar documents.An information retrieval technique using latent semantic structure was patented in 1988 (US Patent 4,839,853, now expired) by Scott Deerwester, Susan Dumais, George Furnas, Richard Harshman, Thomas Landauer, Karen Lochbaum and Lynn Streeter. In the context of its application to information retrieval, it is sometimes called latent semantic indexing (LSI)."})
  ("Artificial intelligence", {"url": "https://en.wikipedia.org/wiki/Artificial_intelligence", "summary": "Artificial intelligence (AI) is intelligence—perceiving, synthesizing, and inferring information—demonstrated by machines, as opposed to intelligence displayed by humans or by other animals. Example tasks in which this is done include speech recognition, computer vision, translation between (natural) languages, as well as other mappings of inputs.\nAI applications include advanced web search engines (e.g., Google Search), recommendation systems (used by YouTube, Amazon, and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Waymo), generative or creative tools (ChatGPT and AI art), automated decision-making, and competing at the highest level in strategic game systems (such as chess and Go).As machines become increasingly capable, tasks considered to require "intelligence" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since it has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an "AI winter"), followed by new approaches, success, and renewed funding. AI research has tried and discarded many different approaches, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge, and imitating animal behavior. In the first decades of the 21st century, highly mathematical and statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field\"s long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability, and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.\nThe field was founded on the assumption that human intelligence "can be so precisely described that a machine can be made to simulate it". This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction, and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals. The term artificial intelligence has also been criticized for overhyping AI\"s true technological capabilities.\n\n"})
  ("Statistics", {"url": "https://en.wikipedia.org/wiki/Statistics", "summary": "Statistics (from German: Statistik, orig. "description of a state, a country") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as "all people living in a country" or "every atom composing a crystal". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\nTwo main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution\"s central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.\nA standard statistical procedure involves the collection of data leading to a test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a "false positive") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a "false negative"). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems."})
  ("Document retrieval", {"url": "https://en.wikipedia.org/wiki/Document_retrieval", "summary": "Document retrieval is defined as the matching of some stated user query against a set of free-text records. These records could be any type of mainly unstructured text, such as newspaper articles, real estate records or paragraphs in a manual. User queries can range from multi-sentence full descriptions of an information need to a few words.\nDocument retrieval is sometimes referred to as, or as a branch of, text retrieval. Text retrieval is a branch of information retrieval where the information is stored primarily in the form of text. Text databases became decentralized thanks to the personal computer. Text retrieval is a critical area of study today, since it is the fundamental basis of all internet search engines."})
  ("Expert system", {"url": "https://en.wikipedia.org/wiki/Expert_system", "summary": "In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.\nExpert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. \nAn expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities."})
  ("Heuristic", {"url": "https://en.wikipedia.org/wiki/Heuristic", "summary": "Heuristic (; from Ancient Greek  εὑρίσκω (heurískō) \"to find, discover\"), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.\nHeuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.\nThe most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification. Here are a few commonly used heuristics from George Pólya\"s 1945 book, How to Solve It:\nIf you are having difficulty understanding a problem, try drawing a picture.\nIf you can\"t find a solution, try assuming that you have a solution and seeing what you can derive from that ("working backward").\nIf the problem is abstract, try examining a concrete example.\nTry solving a more general problem first (the "inventor\"s paradox": the more ambitious plan may have more chances of success).\nIn psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases."})
  ("Knowledge base", {"url": "https://en.wikipedia.org/wiki/Knowledge_base", "summary": "A knowledge base (KB) is a set of sentences, each sentence given in a knowledge representation language, with  interfaces to tell new sentences and to ask questions about what is known, where either of these interfaces might use inference. It is a technology used to store complex structured data  used by a computer system. The initial use of the term was in connection with expert systems, which were the first knowledge-based systems."})
  ("Discourse", {"url": "https://en.wikipedia.org/wiki/Discourse", "summary": "Discourse is a generalization of the notion of a conversation to any form of communication. Discourse is a major topic in social theory, with work spanning fields such as sociology, anthropology, continental philosophy, and discourse analysis. Following pioneering work by Michel Foucault, these fields view discourse as a system of thought, knowledge, or communication that constructs our experience of the world. Since control of discourse amounts to control of how the world is perceived, social theory often studies discourse as a window into power. Within theoretical linguistics, discourse is understood more narrowly as linguistic information exchange and was one of the major motivations for the framework of dynamic semantics, in which expressions" denotations are equated with their ability to update a discourse context."})
  ("Understanding", {"url": "https://en.wikipedia.org/wiki/Understanding", "summary": "Understanding is a psychological process related to an abstract or physical object, such as a person, situation, or message whereby one is able to use concepts to model that object.\nUnderstanding is a relation between the knower and an object of understanding. Understanding implies abilities and dispositions with respect to an object of knowledge that are sufficient to support intelligent behavior.Understanding is often, though not always, related to learning concepts, and sometimes also the theory or theories associated with those concepts. However, a person may have a good ability to predict the behavior of an object, animal or system—and therefore may, in some sense, understand it—without necessarily being familiar with the concepts or theories associated with that object, animal, or system in their culture. They may have developed their own distinct concepts and theories, which may be equivalent, better or worse than the recognized standard concepts and theories of their culture. Thus, understanding is correlated with the ability to make inferences."})
  ("Language", {"url": "https://en.wikipedia.org/wiki/Language", "summary": "Language is a structured system of communication that consists of grammar and vocabulary. It is the primary means by which humans convey meaning, both in spoken and written forms, and may also be conveyed through sign languages. The vast majority of human languages have developed writing systems that allow for the recording and preservation of the sounds or signs of language. Human language is characterized by its cultural and historical diversity, with significant variations observed between cultures and across time. Human languages possess the properties of productivity and displacement, which enable the creation of an infinite number of sentences, and the ability to refer to objects, events, and ideas that are not immediately present in the discourse. The use of human language relies on social convention and is acquired through learning.\nEstimates of the number of human languages in the world vary between 5,000 and 7,000. Precise estimates depend on an arbitrary distinction (dichotomy) established between languages and dialects. Natural languages are spoken, signed, or both; however, any language can be encoded into secondary media using auditory, visual, or tactile stimuli – for example, writing, whistling, signing, or braille. In other words, human language is modality-independent, but written or signed language is the way to inscribe or encode the natural human speech or gestures.\nDepending on philosophical perspectives regarding the definition of language and meaning, when used as a general concept, "language" may refer to the cognitive ability to learn and use systems of complex communication, or to describe the set of rules that makes up these systems, or the set of utterances that can be produced from those rules. All languages rely on the process of semiosis to relate signs to particular meanings. Oral, manual and tactile languages contain a phonological system that governs how symbols are used to form sequences known as words or morphemes, and a syntactic system that governs how words and morphemes are combined to form phrases and utterances.\nThe scientific study of language is called linguistics. Critical examinations of languages, such as philosophy of language, the relationships between language and thought, how words represent experience, etc., have been debated at least since Gorgias and Plato in ancient Greek civilization. Thinkers such as Jean-Jacques Rousseau (1712–1778) have argued that language originated from emotions, while others like Immanuel Kant (1724–1804) have argued that languages originated from rational and logical thought. Twentieth century philosophers such as Ludwig Wittgenstein (1889–1951) argued that philosophy is really the study of language itself. Major figures in contemporary linguistics of these times include Ferdinand de Saussure and Noam Chomsky.\nLanguage is thought to have gradually diverged from earlier primate communication systems when early hominins acquired the ability to form a theory of mind and shared intentionality. This development is sometimes thought to have coincided with an increase in brain volume, and many linguists see the structures of language as having evolved to serve specific communicative and social functions. Language is processed in many different locations in the human brain, but especially in Broca\"s and Wernicke\"s areas. Humans acquire language through social interaction in early childhood, and children generally speak fluently by approximately three years old. Language and culture are codependent. Therefore, in addition to its strictly communicative uses, language has social uses such as signifying group identity, social stratification, as well as use for social grooming and entertainment.\nLanguages evolve and diversify over time, and the history of their evolution can be reconstructed by comparing modern languages to determine which traits their ancestral languages must have had in order for the later developmental stages to occur. A group of languages that descend from a common ancestor is known as a language family; in contrast, a language that has been demonstrated to not have any living or non-living relationship with another language is called a language isolate. There are also many unclassified languages whose relationships have not been established, and spurious languages may have not existed at all. Academic consensus holds that between 50% and 90% of languages spoken at the beginning of the 21st century will probably have become extinct by the year 2100."})
  ("Information overload", {"url": "https://en.wikipedia.org/wiki/Information_overload", "summary": "Information overload (also known as infobesity, infoxication, information anxiety, and information explosion) is the difficulty in understanding an issue and effectively making decisions when one has too much information (TMI) about that issue, and is generally associated with the excessive quantity of daily information. The term "information overload" was first used as early as 1962 by scholars in management and information studies, including in Bertram Gross\" 1964 book, The Managing of Organizations, and was further popularized by Alvin Toffler in his bestselling 1970 book Future Shock. Speier et al. (1999) said that if input exceeds the processing capacity, information overload occurs, which is likely to reduce the quality of the decisions.In a newer definition, Roetzel (2019) focuses on time and resources aspects. He states that when a decision-maker is given many sets of information, such as complexity, amount, and contradiction, the quality of its decision is decreased because of the individual’s limitation of scarce resources to process all the information and optimally make the best decision.The advent of modern information technology has been a primary driver of information overload on multiple fronts: in quantity produced, ease of dissemination, and breadth of the audience reached. Longstanding technological factors have been further intensified by the rise of social media and the attention economy, which facilitates attention theft. In the age of connective digital technologies, informatics, the Internet culture (or the digital culture), information overload is associated with over-exposure, excessive viewing of information, and input abundance of information and data."})
  ("The", {"url": "https://en.wikipedia.org/wiki/The", "summary": "The ( (listen)) is a grammatical article in English, denoting persons or things already mentioned, under discussion, implied or otherwise presumed familiar to listeners, readers, or speakers. It is the definite article in English. The is the most frequently used word in the English language; studies and analyses of texts have found it to account for seven percent of all printed English-language words. It is derived from gendered articles in Old English which combined in Middle English and now has a single form used with nouns of any gender. The word can be used with both singular and plural nouns, and with a noun that starts with any letter. This is different from many other languages, which have different forms of the definite article for different genders or numbers."})
  ("Stop word", {"url": "https://en.wikipedia.org/wiki/Stop_word", "summary": "Stop words are the words in a stop list (or stoplist or negative dictionary) which are filtered out (i.e. stopped) before or after processing of natural language data (text) because they are insignificant. There is no single universal list of stop words used by all natural language processing tools, nor any agreed upon rules for identifying stop words, and indeed not all tools even use such a list. Therefore, any group of words can be chosen as the stop words for a given purpose. The "general trend in [information retrieval]  systems over time has been from standard use of quite large stop lists (200–300 terms) to very small stop lists (7–12 terms) to no stop list whatsoever".\n\n"})
  ("Gensim", {"url": "https://en.wikipedia.org/wiki/Gensim", "summary": "Gensim is an open-source library for unsupervised topic modeling, document indexing, retrieval by similarity, and other natural language processing functionalities, using modern statistical machine learning.\nGensim is implemented in Python and Cython for performance. Gensim is designed to handle large text collections using data streaming and incremental online algorithms, which differentiates it from most other machine learning software packages that target only in-memory processing."})
  ("Natural Language Toolkit", {"url": "https://en.wikipedia.org/wiki/Natural_Language_Toolkit", "summary": "The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania. NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit, plus a cookbook.NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.\nNLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.\n\n"})
  ("Abbreviation", {"url": "https://en.wikipedia.org/wiki/Abbreviation", "summary": "An abbreviation (from Latin breviscode: lat promoted to code: la , meaning short) is a shortened form of a word or phrase, by any method. It may consist of a group of letters or words taken from the full version of the word or phrase; for example, the word abbreviation can itself be represented by the abbreviation abbr., abbrv., or abbrev.; NPO, for nil (or nothing) per (by) os (mouth) is an abbreviated medical instruction. It may also consist of initials only, a mixture of initials and words, or words or letters representing words in another language (for example, e.g., i.e. or RSVP). Some types of abbreviations are acronyms (some pronounceable, some initialisms) or grammatical contractions or crasis.\nAn abbreviation is a shortening by any of these or other methods.\n\n"})
  ("Spelling", {"url": "https://en.wikipedia.org/wiki/Spelling", "summary": "Spelling is a set of conventions that regulate the way of using graphemes (writing system) to represent a language in its written form. In other words, spelling is the rendering of speech sound (phoneme) into writing (grapheme). Spelling is one of the elements of orthography, and highly standardized spelling is a prescriptive element.\nSpellings originated as transcriptions of the sounds of spoken language according to the alphabetic principle. They remain largely reflective of the sounds, although fully phonemic spelling is an ideal that most languages" orthographies only approximate, some more closely than others. This is true for various reasons, including that pronunciation changes over time in all languages, yet spellings as visual norms may resist change. In addition, words from other languages may be adopted without being adapted to the spelling system, and different meanings of a word or homophones may be deliberately spelled in different ways to differentiate them visually."})
  ("Stemming", {"url": "https://en.wikipedia.org/wiki/Stemming", "summary": "In linguistic morphology and information retrieval, stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root. Algorithms for stemming have been studied in computer science since the 1960s. Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.\nA computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer."})
  ("Parsing", {"url": "https://en.wikipedia.org/wiki/Parsing", "summary": "Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.  The term parsing comes from Latin pars (orationis), meaning part (of speech).The term has slightly different meanings in different branches of linguistics and computer science.  Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, sometimes with the aid of devices such as sentence diagrams.  It usually emphasizes the importance of grammatical divisions such as subject and predicate.\nWithin computational linguistics the term is used to refer to the formal analysis by a computer of a sentence or other string of words into its constituents, resulting in a parse tree showing their syntactic relation to each other, which may also contain semantic and other information (p-values). Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.The term is also used in psycholinguistics when describing language comprehension.  In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, identifying the parts of speech, syntactic relations, etc."  This term is especially common when discussing which linguistic cues help speakers interpret garden-path sentences.\nWithin computer science, the term is used in the analysis of computer languages, referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. The term may also be used to describe a split or separation.\n\n"})
  ("PDF", {"url": "https://en.wikipedia.org/wiki/PDF", "summary": "Portable Document Format (PDF), standardized as ISO 32000, is a file format developed by Adobe in 1992 to present documents, including text formatting and images, in a manner independent of application software, hardware, and operating systems. Based on the PostScript language, each PDF file encapsulates a complete description of a fixed-layout flat document, including the text, fonts, vector graphics, raster images and other information needed to display it. PDF has its roots in "The Camelot Project" initiated by  Adobe co-founder John Warnock in 1991.PDF was standardized as ISO 32000 in 2008. The last edition as ISO 32000-2:2020 was published in December 2020.\nPDF files may contain a variety of content besides flat text and graphics including logical structuring elements, interactive elements such as annotations and form-fields, layers, rich media (including video content), three-dimensional objects using U3D or PRC, and various other data formats. The PDF specification also provides for encryption and digital signatures, file attachments, and metadata to enable workflows requiring these features.\n\n"})
  ("Generative adversarial network", {"url": "https://en.wikipedia.org/wiki/Generative_adversarial_network", "summary": "A generative adversarial network (GAN) is a class of machine learning frameworks and a prominent framework for approaching generative AI. The concept was initially developed by Ian Goodfellow and his colleagues in June 2014. In a GAN, two neural networks contest with each other in the form of a zero-sum game, where one agent\"s gain is another agent\"s loss.\nGiven a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the "indirect" training through the discriminator, another neural network that can tell how "realistic" the input seems, which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.\nGANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks."})
  ("Discriminative model", {"url": "https://en.wikipedia.org/wiki/Discriminative_model", "summary": "Discriminative models, also referred to as conditional models, are a class of logistical models used for classification or regression. They distinguish decision boundaries through observed data, such as pass/fail, win/lose, alive/dead or healthy/sick.\nTypical discriminative models include logistic regression (LR), conditional random fields (CRFs) (specified over an undirected graph), decision trees, and many others. Typical generative model approaches include naive Bayes classifiers, Gaussian mixture models, variational autoencoders, generative adversarial networks and others."})
  ("Generative model", {"url": "https://en.wikipedia.org/wiki/Generative_model", "summary": "In statistical classification, two main approaches are called the generative approach and the discriminative approach. These compute classifiers by different approaches, differing in the degree of statistical modelling. Terminology is inconsistent, but three major types can be distinguished, following Jebara (2004):\n\nA generative model is a statistical model of the joint probability distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   on given observable variable X and target variable Y;\nA discriminative model is a model of the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        ∣\n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y\\mid X=x)}\n   of the target Y, given an observation x; and\nClassifiers computed without using a probability model are also referred to loosely as "discriminative".The distinction between these last two classes is not consistently made; Jebara (2004) refers to these three classes as generative learning, conditional learning, and discriminative learning, but Ng & Jordan (2002) only distinguish two classes, calling them generative classifiers (joint distribution) and discriminative classifiers (conditional distribution or no distribution), not distinguishing between the latter two classes. Analogously, a classifier based on a generative model is a generative classifier, while a classifier based on a discriminative model is a discriminative classifier, though this term also refers to classifiers that are not based on a model.\nStandard examples of each, all of which are linear classifiers, are:\n\ngenerative classifiers:\nnaive Bayes classifier and\nlinear discriminant analysis\ndiscriminative model:\nlogistic regressionIn application to classification, one wishes to go from an observation x to a label y (or probability distribution on labels). One can compute this directly, without using a probability distribution (distribution-free classifier); one can estimate the probability of a label given an observation, \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n   (discriminative model), and base classification on that; or one can estimate the joint distribution \n  \n    \n      \n        P\n        (\n        X\n        ,\n        Y\n        )\n      \n    \n    {\\displaystyle P(X,Y)}\n   (generative model), from that compute the conditional probability \n  \n    \n      \n        P\n        (\n        Y\n        \n          |\n        \n        X\n        =\n        x\n        )\n      \n    \n    {\\displaystyle P(Y|X=x)}\n  , and then base classification on that. These are increasingly indirect, but increasingly probabilistic, allowing more domain knowledge and probability theory to be applied. In practice different approaches are used, depending on the particular problem, and hybrids can combine strengths of multiple approaches."})
  ("Model", {"url": "https://en.wikipedia.org/wiki/Model", "summary": "A model is an informative representation of an object, person or system. The term originally denoted the plans of a building in late 16th-century English, and derived via French and Italian ultimately from Latin modulus, a measure. \nModels can be divided into physical models (e.g. a model plane) and abstract models (e.g. mathematical expressions describing behavioural patterns). Abstract or conceptual models are central to philosophy of science, as almost every scientific theory effectively embeds some kind of model of the physical or human sphere. \nIn commerce, "model" can refer to a specific design of a product as displayed in a catalogue or show room (e.g. Ford Model T), and by extension to the sold product itself.\nTypes of models include:"})
  ("Graphics processing unit", {"url": "https://en.wikipedia.org/wiki/Graphics_processing_unit", "summary": "A graphics processing unit (GPU) is a specialized electronic circuit designed to manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.\nModern GPUs are efficient at manipulating computer graphics and image processing. Their parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In some CPUs, they are embedded on the CPU die."})
  ("Normal distribution", {"url": "https://en.wikipedia.org/wiki/Normal_distribution", "summary": "In statistics, a normal distribution or Gaussian distribution is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          \n            1\n            \n              σ\n              \n                \n                  2\n                  π\n                \n              \n            \n          \n        \n        \n          e\n          \n            −\n            \n              \n                1\n                2\n              \n            \n            \n              \n                (\n                \n                  \n                    \n                      x\n                      −\n                      μ\n                    \n                    σ\n                  \n                \n                )\n              \n              \n                2\n              \n            \n          \n        \n      \n    \n    {\\displaystyle f(x)={\\frac {1}{\\sigma {\\sqrt {2\\pi }}}}e^{-{\\frac {1}{2}}\\left({\\frac {x-\\mu }{\\sigma }}\\right)^{2}}}\n  The parameter \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n   is the mean or expectation of the distribution (and also its median and mode), while the parameter \n  \n    \n      \n        σ\n      \n    \n    {\\displaystyle \\sigma }\n   is its standard deviation. The variance of the distribution is \n  \n    \n      \n        \n          σ\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\sigma ^{2}}\n  . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.\nNormal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.\nA normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student"s t, and logistic distributions).  For other names, see Naming.\nThe univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution."})
  ("Normality (behavior)", {"url": "https://en.wikipedia.org/wiki/Normality_(behavior)", "summary": "Normality is a behavior that can be normal for an individual (intrapersonal normality) when it is consistent with the most common behavior for that person. Normal is also used to describe individual behavior that conforms to the most common behavior in society (known as conformity). However, normal behavior is often only recognized in contrast to abnormality. In many cases normality is used to make moral judgements, such that normality is seen as good while abnormality is seen as bad, or conversely normality can seen as boring and uninteresting. Someone being seen as normal or not normal can have social ramifications, such as being included, excluded or stigmatized by wider society."})
  ("Machine learning", {"url": "https://en.wikipedia.org/wiki/Machine_learning", "summary": "Machine learning (ML) is a field devoted to understanding and building methods that let machines "learn" – that is, methods that leverage data to improve computer performance on some set of tasks.Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, agriculture, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.In its application across business problems, machine learning is also referred to as predictive analytics."})
  ("Unsupervised learning", {"url": "https://en.wikipedia.org/wiki/Unsupervised_learning", "summary": "Unsupervised learning refers to algorithms that learn patterns from unlabeled data.\nIn contrast to supervised learning where models learn to map the input to the target output (e.g. images labeled as a "cat" or "fish"), unsupervised methods learn concise representations of the input data, which can be used for data exploration or to analyze or generate new data. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a "performance score" as guidance, and semi-supervised learning where only a portion of training data is labeled."})
  ("Unsupervised", {"url": "https://en.wikipedia.org/wiki/Unsupervised", "summary": "Unsupervised is an American adult animated sitcom created by David Hornsby, Rob Rosell, and Scott Marder which ran on FX from January 19 to December 20, 2012. The show was created, and for the most part, written by David Hornsby, Scott Marder, and Rob Rosell.On November 17, 2012, the series was canceled after one season."})
  ("Posterior probability", {"url": "https://en.wikipedia.org/wiki/Posterior_probability", "summary": "The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood via an application of Bayes" rule. From an epistemological perspective, the posterior probability contains everything there is to know about an uncertain proposition (such as a scientific hypothesis, or parameter values), given prior knowledge and a mathematical model describing the observations available at a particular time. After the arrival of new information, the current posterior probability may serve as the prior in another round of Bayesian updating.In the context of Bayesian statistics, the posterior probability distribution usually describes the epistemic uncertainty about statistical parameters conditional on a collection of observed data. From a given posterior distribution, various point and interval estimates can be derived, such as the maximum a posteriori (MAP) or the highest posterior density interval (HPDI). But while conceptually simple, the posterior distribution is generally not tractable and therefore needs to be either analytically or numerically approximated."})
  ("Hidden Markov model", {"url": "https://en.wikipedia.org/wiki/Hidden_Markov_model", "summary": "A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process — call it \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   — with unobservable ("hidden") states. As part of the definition, HMM requires that there be an observable process \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   whose outcomes are "influenced" by the outcomes of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   in a known way. Since \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   cannot be observed directly, the goal is to learn about \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   by observing \n  \n    \n      \n        Y\n        .\n      \n    \n    {\\displaystyle Y.}\n   HMM has an additional requirement that the outcome of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at time \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   must be "influenced" exclusively by the outcome of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   at \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   and that the outcomes of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at \n  \n    \n      \n        t\n        <\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t<t_{0}}\n   must be conditionally independent of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   at \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle t=t_{0}}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   at time \n  \n    \n      \n        t\n        =\n        \n          t\n          \n            0\n          \n        \n        .\n      \n    \n    {\\displaystyle t=t_{0}.}\n  \nHidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics."})
  ("Bayesian network", {"url": "https://en.wikipedia.org/wiki/Bayesian_network", "summary": "A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.\nEfficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\n"})
  ("Training", {"url": "https://en.wikipedia.org/wiki/Training", "summary": "Training is teaching, or developing in oneself or others, any skills and knowledge  or fitness that relate to specific useful competencies. Training has specific goals of improving one"s capability, capacity, productivity and performance. It forms the core of apprenticeships and provides the backbone of content at institutes of technology (also known as technical colleges or polytechnics). In addition to the basic training required for a trade, occupation or profession, training may continue beyond initial competence to maintain, upgrade and update skills throughout working life. People within some professions and occupations may refer to this sort of training as professional development. Training also refers to the development of physical fitness related to a specific competence, such as sport, martial arts, military applications and some other occupations.\n\n"})
  ("Dataspaces", {"url": "https://en.wikipedia.org/wiki/Dataspaces", "summary": "Dataspaces are an abstraction in data management that aim to overcome some of the problems encountered in data integration system. The aim is to reduce the effort required to set up a data integration system by relying on existing matching and mapping generation techniques, and to improve the system in "pay-as-you-go" fashion as it is used. Labor-intensive aspects of data integration are postponed until they are absolutely needed.Traditionally, data integration and data exchange systems have aimed to offer many of the purported services of dataspace systems.\nDataspaces can be viewed as a next step in the evolution of data integration architectures, but are distinct from current data integration systems in the following way. Data integration systems require semantic integration before any services can be provided. Hence, although there is not a single schema to which all the data conforms and the data resides in a multitude of host systems, the data integration system knows the precise relationships between the terms used in each schema. As a result, significant up-front effort is required in order to set up a data integration system.\nDataspaces shift the emphasis to a data co-existence approach providing base functionality over all data sources, regardless of how integrated they are. For example, a DataSpace Support Platform (DSSP) can provide keyword search over all of its data sources, similar to that provided by existing desktop search systems. When more sophisticated operations are required, such as relational-style queries, data mining, or monitoring over certain sources, then additional effort can be applied to more closely integrate those sources in an incremental fashion. Similarly, in terms of traditional database guarantees, initially a dataspace system can only provide weaker guarantees of consistency and durability. As stronger guarantees are desired, more effort can be put into making agreements among the various owners of data sources, and opening up certain interfaces (e.g., for commit protocols).\nData graphs play an important role in dataspaces systems. They work on a fact based (triples or "data entities" made up of subject-predicate-object) data modeling approach which supports the "pay-as-you-go" techniques described above. They support data co-existence and are therefore an ideal technique for semantic integration. Search and relational-style queries and analytics can work simultaneously on data graphs which is another important property of dataspaces."})
  ("Data", {"url": "https://en.wikipedia.org/wiki/Data", "summary": "In the pursuit of knowledge, data (US: ; UK: ) is a collection of discrete values that convey information, describing quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted.  A datum is an individual value in a collection of data. Data  is usually organized into structures such as tables that provide additional context and meaning, and which may themselves be used as data in larger structures. Data may be used as variables in a computational process. Data may represent abstract ideas or concrete measurements.\nData is commonly used in scientific research, economics, and in virtually every other form of human organizational activity.  Examples of data sets include price indices (such as consumer price index), unemployment rates, literacy rates, and census data. In this context, data represents the raw facts and figures which can be used in such a manner in order to capture the useful information out of it. \nData is collected using techniques such as measurement, observation, query, or analysis, and typically represented as numbers or characters which may be further processed. Field data is data that is collected in an uncontrolled in-situ environment. Experimental data is data that is generated in the course of a controlled scientific experiment.  Data is analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis.  Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed and obvious instrument or data entry errors are corrected.\nData can be seen as the smallest units of factual information that can be used as a basis for calculation, reasoning, or discussion. Data can range from abstract ideas to concrete measurements, including but not limited to, statistics. Thematically connected data presented in some relevant context can be viewed as information. Contextually connected pieces of information can then be described as data insights or intelligence. The stock of insights and intelligence that accumulates over time resulting from the synthesis of data into information, can then be described as knowledge. Data has been described as "the new oil of the digital economy". Data, as a general concept, refers to the fact that some existing information or knowledge is represented or coded in some form suitable for better usage or processing.\nAdvances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence (AI)) methods that allow for efficient applications of analytic methods to big data."})
  ("Mathematics", {"url": "https://en.wikipedia.org/wiki/Mathematics", "summary": "Mathematics is an area of knowledge that includes the topics of numbers, formulas and related structures, shapes and the spaces in which they are contained, and quantities and their changes. These topics are represented in modern mathematics with the major subdisciplines of number theory, algebra, geometry, and analysis, respectively. There is no general consensus among mathematicians about a common definition for their academic discipline.\nMost mathematical activity involves the discovery of properties of abstract objects and the use of pure reason to prove them. These objects consist of either abstractions from nature or—in modern mathematics—entities that are stipulated to have certain properties, called axioms. A proof consists of a succession of applications of deductive rules to already established results. These results include previously proved theorems, axioms, and—in case of abstraction from nature—some basic properties that are considered true starting points of the theory under consideration.Mathematics is essential in the natural sciences, engineering, medicine, finance, computer science and the social sciences. Although mathematics is extensively used for modeling phenomena, the fundamental truths of mathematics are independent from any scientific experimentation. Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other areas are developed independently from any application (and are therefore called pure mathematics), but often later find practical applications. The problem of integer factorization, for example, which goes back to Euclid in 300 BC, had no practical application before its use in the RSA cryptosystem, now widely used for the security of computer networks.\nHistorically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid"s Elements. Since its beginning, mathematics was essentially divided into geometry and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid lockstep increase in the development of both. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method, which heralded a dramatic increase in the number of mathematical areas and their fields of application. The contemporary Mathematics Subject Classification lists more than 60 first-level areas of mathematics.\n\n"})
  ("Supervised learning", {"url": "https://en.wikipedia.org/wiki/Supervised_learning", "summary": "Supervised learning (SL) is a machine learning paradigm for problems where the available data consists of labeled examples, meaning that each data point contains features (covariates) and an associated label. The goal of supervised learning algorithms is learning a function that maps feature vectors (inputs) to labels (output), based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error."})
  ("Train", {"url": "https://en.wikipedia.org/wiki/Train", "summary": "A train (from Old French trahiner, from Latin trahere, "to pull, to draw") is a series of connected vehicles that run along a railway track and transport people or freight. Trains are typically pulled or pushed by locomotives (often known simply as "engines"), though some are self-propelled, such as multiple units. Passengers and cargo are carried in railroad cars, also known as wagons. Trains are designed to a certain gauge, or distance between rails. Most trains operate on steel tracks with steel wheels, the low friction of which makes them more efficient than other forms of transport.\nTrains have their roots in wagonways, which used railway tracks and were powered by horses or pulled by cables. Following the invention of the steam locomotive in the United Kingdom in 1804, trains rapidly spread around the world, allowing freight and passengers to move over land faster and cheaper than ever possible before. Rapid transit and trams were first built in the late 1800s to transport large numbers of people in and around cities. Beginning in the 1920s, and accelerating following World War II, diesel and electric locomotives replaced steam as the means of motive power. Following the development of cars, trucks, and extensive networks of highways which offered greater mobility, as well as faster airplanes, trains declined in importance and market share, and many rail lines were abandoned. The spread of buses led to the closure of many rapid transit and tram systems during this time as well.\nSince the 1970s, governments, environmentalists, and train advocates have promoted increased use of trains due to their greater fuel efficiency and lower greenhouse gas emissions compared to other modes of land transport. High-speed rail, first built in the 1960s, has proven competitive with cars and planes over short to medium distances. Commuter rail has grown in importance since the 1970s as an alternative to congested highways and a means to promote development, as has light rail in the 21st century. Freight trains remain important for the transport of bulk commodities such as coal and grain, as well as being a means of reducing road traffic congestion by freight trucks.\nWhile conventional trains operate on relatively flat tracks with two rails, a number of specialized trains exist which are significantly different in their mode of operation. Monorails operate on a single rail, while funiculars and rack railways are uniquely designed to traverse steep slopes. Experimental trains such as high speed maglevs, which use magnetic levitation to float above a guideway, are under development in the 2020s and offer higher speeds than even the fastest conventional trains. Trains which use alternative fuels such as natural gas and hydrogen are another 21st century development."})
  ("Statistical classification", {"url": "https://en.wikipedia.org/wiki/Statistical_classification", "summary": "In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the "spam" or "non-spam" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type), ordinal (e.g. "large", "medium" or "small"), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\nAn algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term "classifier" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.\nTerminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term "classification" normally refers to cluster analysis."})
  ("Probability theory", {"url": "https://en.wikipedia.org/wiki/Probability_theory", "summary": "Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.\nCentral subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes (which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion).\nAlthough it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.\nAs a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics. \n\n"})
  ("Continuous or discrete variable", {"url": "https://en.wikipedia.org/wiki/Continuous_or_discrete_variable", "summary": "In mathematics and statistics, a quantitative variable may be continuous or discrete if they are typically obtained by measuring or counting, respectively. If it can take on two particular real values such that it can also take on all real values between them (even values that are arbitrarily close together), the variable is continuous in that interval. If it can take on a value such that there is a non-infinitesimal gap on each side of it containing no values that the variable can take on, then it is discrete around that value.   In some contexts a variable can be discrete in some ranges of the number line and continuous in others."})
  ("Label", {"url": "https://en.wikipedia.org/wiki/Label", "summary": "A label (as distinct from signage) is a piece of paper, plastic film, cloth, metal, or other material affixed to a container or product, on which is written or printed information or symbols about the product or item. Information printed directly on a container or article can also be considered labelling.\nLabels have many uses, including promotion and providing information on a product"s origin, the manufacturer (e.g., brand name), use, safety, shelf-life and disposal, some or all of which may be governed by legislation such as that for food in the UK or United States. Methods of production and attachment to packaging are many and various and may also be subject to internationally recognised standards. In many countries, hazardous products such as poisons or flammable liquids must have a warning label."})
  ("Classification", {"url": "https://en.wikipedia.org/wiki/Classification", "summary": "Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. \nClassification is the grouping of related facts into classes. \nIt may also refer to:"})
  ("Conditional probability distribution", {"url": "https://en.wikipedia.org/wiki/Conditional_probability_distribution", "summary": "In probability theory and statistics, given two jointly distributed random variables \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n  , the conditional probability distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is the probability distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   when \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is known to be a particular value; in some cases the conditional probabilities may be expressed as functions containing the unspecified value \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   of \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   as a parameter. When both \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   and \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   are categorical variables, a conditional probability table is typically used to represent the conditional probability. The conditional distribution contrasts with the marginal distribution of a random variable, which is its distribution without reference to the value of the other variable.\nIf the conditional distribution of \n  \n    \n      \n        Y\n      \n    \n    {\\displaystyle Y}\n   given \n  \n    \n      \n        X\n      \n    \n    {\\displaystyle X}\n   is a continuous distribution, then its probability density function is known as the conditional density function. The properties of a conditional distribution, such as the moments, are often referred to by corresponding names such as the conditional mean and conditional variance.\nMore generally, one can refer to the conditional distribution of a subset of a set of more than two variables; this conditional distribution is contingent on the values of all the remaining variables, and if more than one variable is included in the subset then this conditional distribution is the conditional joint distribution of the included variables.\n\n"})
  ("Marginal distribution", {"url": "https://en.wikipedia.org/wiki/Marginal_distribution", "summary": "In probability theory and statistics, the marginal distribution of a subset of a collection of random variables is the probability distribution of the variables contained in the subset. It gives the probabilities of various values of the variables in the subset without reference to the values of the other variables. This contrasts with a conditional distribution, which gives the probabilities contingent upon the values of the other variables.\nMarginal variables are those variables in the subset of variables being retained. These concepts are "marginal" because they can be found by summing values in a table along rows or columns, and writing the sum in the margins of the table. The distribution of the marginal variables (the marginal distribution) is obtained by marginalizing (that is, focusing on the sums in the margin) over the distribution of the variables being discarded, and the discarded variables are said to have been marginalized out.\nThe context here is that the theoretical studies being undertaken, or the data analysis being done, involves a wider set of random variables but that attention is being limited to a reduced number of those variables. In many applications, an analysis may start with a given collection of random variables, then first extend the set by defining new ones (such as the sum of the original random variables) and finally reduce the number by placing interest in the marginal distribution of a subset (such as the sum). Several different analyses may be done, each treating a different subset of variables as the marginal distribution."})
  ("Deep learning", {"url": "https://en.wikipedia.org/wiki/Deep_learning", "summary": "Deep learning is part of a broader family of machine learning methods, which is based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks, convolutional neural networks and transformers have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analog.The adjective "deep" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation that is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability."})
  ("Jukebox", {"url": "https://en.wikipedia.org/wiki/Jukebox", "summary": "A jukebox is a partially automated music-playing device, usually a coin-operated machine, that will play a patron"s selection from self-contained media. The classic jukebox has buttons, with letters and numbers on them, which are used to select a specific record. Some may use compact discs instead. Disc changers are similar devices that are intended for home use, are small enough to fit in a shelf, may hold up to hundreds of discs, and allow discs to be easily removed, replaced, and inserted by the user."})
  ("Search engine", {"url": "https://en.wikipedia.org/wiki/Search_engine", "summary": "A web search engine is a software system designed to carry out web searches.  They search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). When a user enters a query into a search engine, the engine scans its index of web pages to find those that are relevant to the user"s query. The results are then ranked by relevancy and displayed to the user. The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories and social bookmarking sites, which are maintained by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Any internet-based content that cannot be indexed and searched by a web search engine falls under the category of deep web."})
  ("Query likelihood model", {"url": "https://en.wikipedia.org/wiki/Query_likelihood_model", "summary": "The query likelihood model is a language model used in information retrieval. A language model is constructed for each document in the collection. It is then possible to rank each document by the probability of specific documents given a query. This is interpreted as being the likelihood of a document being relevant given a query."})
  ("Size", {"url": "https://en.wikipedia.org/wiki/Size", "summary": "Size in general is the magnitude or dimensions of a thing. More specifically, geometrical size (or spatial size) can refer to linear dimensions (length, width, height, diameter, perimeter), area, or volume. Size can also be measured in terms of mass, especially when assuming a density range.\n\nIn mathematical terms, "size is a concept abstracted from the process of measuring by comparing a longer to a shorter". Size is determined by the process of comparing or measuring objects, which results in the determination of the magnitude of a quantity, such as length or mass, relative to a unit of measurement. Such a magnitude is usually expressed as a numerical value of units on a previously established spatial scale, such as meters or inches.\nThe sizes with which humans tend to be most familiar are body dimensions (measures of anthropometry), which include measures such as human height and human body weight. These measures can, in the aggregate, allow the generation of commercially useful distributions of products that accommodate expected body sizes, as with the creation of clothing sizes and shoe sizes, and with the standardization of door frame dimensions, ceiling heights, and bed sizes. The human experience of size can lead to a psychological tendency towards size bias, wherein the relative importance or perceived complexity of organisms and other objects is judged based on their size relative to humans, and particularly whether this size makes them easy to observe without aid."})
  ("Hypothesis", {"url": "https://en.wikipedia.org/wiki/Hypothesis", "summary": "A hypothesis (plural hypotheses) is a proposed explanation for a phenomenon. For a hypothesis to be a scientific hypothesis, the scientific method requires that one can test it. Scientists generally base scientific hypotheses on previous observations that cannot satisfactorily be explained with the available scientific theories. Even though the words "hypothesis" and "theory" are often used interchangeably, a scientific hypothesis is not the same as a scientific theory. A working hypothesis is a provisionally accepted hypothesis proposed for further research in a process beginning with an educated guess or thought.A different meaning of the term hypothesis is used in formal logic, to denote the antecedent of a proposition; thus in the proposition "If P, then Q", P denotes the hypothesis (or antecedent); Q can be called a consequent. P is the assumption in a (possibly counterfactual) What If question.\nThe adjective hypothetical, meaning "having the nature of a hypothesis", or "being assumed to exist as an immediate consequence of a hypothesis", can refer to any of these meanings of the term "hypothesis".\n\n"})
  ("Minimally invasive procedure", {"url": "https://en.wikipedia.org/wiki/Minimally_invasive_procedure", "summary": "Minimally invasive procedures (also known as minimally invasive surgeries) encompass surgical techniques that limit the size of incisions needed, thereby reducing wound healing time, associated pain, and risk of infection. Surgery by definition is invasive and many operations requiring incisions of some size are referred to as open surgery. Incisions made during open surgery can sometimes leave large wounds that may be painful and take a long time to heal. Advancements in medical technologies have enabled the development and regular use of minimally invasive procedures. For example, endovascular aneurysm repair, a minimally invasive surgery, has become the most common method of repairing abdominal aortic aneurysms in the US as of 2003. The procedure involves much smaller incisions than the corresponding open surgery procedure of open aortic surgery.Interventional radiologists were the forerunners of minimally invasive procedures. Using imaging techniques, radiologists were able to direct interventional instruments through the body by way of catheters instead of the large incisions needed in traditional surgery. As a result, many conditions once requiring surgery can now be treated non-surgically.Diagnostic techniques that do not involve incisions, puncturing the skin, or the introduction of foreign objects or materials into the body are known as non-invasive procedures. Several treatment procedures are classified as non-invasive. A major example of a non-invasive alternative treatment to surgery is radiation therapy, also called radiotherapy."})
  ("Unit of observation", {"url": "https://en.wikipedia.org/wiki/Unit_of_observation", "summary": "In statistics, a unit of observation is the unit described by the data that one analyzes. A study may treat groups as a unit of observation with a country as the unit of analysis, drawing conclusions on group characteristics from data collected at the national level. For example, in a study of the demand for money, the unit of observation might be chosen as the individual, with different observations (data points) for a given point in time differing as to which individual they refer to; or the unit of observation might be the country, with different observations differing only in regard to the country they refer to."})
  ("Exponential growth", {"url": "https://en.wikipedia.org/wiki/Exponential_growth", "summary": "Exponential growth is a process that increases quantity over time. It occurs when the instantaneous rate of change (that is, the derivative) of a quantity with respect to time is proportional to the quantity itself. Described as a function, a quantity undergoing exponential growth is an exponential function of time, that is, the variable representing time is the exponent (in contrast to other types of growth, such as quadratic growth).\nIf the constant of proportionality is negative, then the quantity decreases over time, and is said to be undergoing exponential decay instead. In the case of a discrete domain of definition with equal intervals, it is also called geometric growth or geometric decay since the function values form a geometric progression.\nThe formula for exponential growth of a variable x at the growth rate r, as time t goes on in discrete intervals (that is, at integer times 0, 1, 2, 3, ...), is\n\nwhere x0 is the value of x at time 0. The growth of a bacterial colony is often used to illustrate it. One bacterium splits itself into two, each of which splits itself resulting in four, then eight, 16, 32, and so on. The amount of increase keeps increasing because it is proportional to the ever-increasing number of bacteria. Growth like this is observed in real-life activity or phenomena, such as the spread of virus infection, the growth of debt due to compound interest, and the spread of viral videos. In real cases, initial exponential growth often does not last forever, instead slowing down eventually due to upper limits caused by external factors and turning into logistic growth.\nTerms like "exponential growth" are sometimes incorrectly interpreted as "rapid growth". Indeed, something that grows exponentially can in fact be growing slowly at first.\n\n"})
  ("Least squares", {"url": "https://en.wikipedia.org/wiki/Least_squares", "summary": "The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.\nThe most important application is in data fitting. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares.\nLeast squares problems fall into two categories: linear or ordinary least squares and nonlinear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.\nPolynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.\nWhen the observations come from an exponential family with identity as its natural sufficient statistics and mild-conditions are satisfied (e.g. for normal, exponential, Poisson and binomial distributions), standardized least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator.\nThe following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model.\nThe least-squares method was officially discovered and published by Adrien-Marie Legendre (1805), though it is usually also co-credited to Carl Friedrich Gauss (1795) who contributed significant theoretical advances to the method and may have previously used it in his work."})
  ("Linear regression", {"url": "https://en.wikipedia.org/wiki/Linear_regression", "summary": "In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.\nLinear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n\nIf the goal is error reduction in prediction or forecasting, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.\nIf the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the "lack of fit" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms "least squares" and "linear model" are closely linked, they are not synonymous."})
  ("Logit", {"url": "https://en.wikipedia.org/wiki/Logit", "summary": "In statistics, the logit ( LOH-jit) function is the quantile function associated with the standard logistic distribution. It has many uses in data analysis and machine learning, especially in data transformations.\nMathematically, the logit is the inverse of the standard logistic function \n  \n    \n      \n        σ\n        (\n        x\n        )\n        =\n        1\n        \n          /\n        \n        (\n        1\n        +\n        \n          e\n          \n            −\n            x\n          \n        \n        )\n      \n    \n    {\\displaystyle \\sigma (x)=1/(1+e^{-x})}\n  , so the logit is defined as\n\n  \n    \n      \n        logit\n        \u2061\n        p\n        =\n        \n          σ\n          \n            −\n            1\n          \n        \n        (\n        p\n        )\n        =\n        ln\n        \u2061\n        \n          \n            p\n            \n              1\n              −\n              p\n            \n          \n        \n        \n        \n          for\n        \n        \n        p\n        ∈\n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle \\operatorname {logit} p=\\sigma ^{-1}(p)=\\ln {\\frac {p}{1-p}}\\quad {\\text{for}}\\quad p\\in (0,1)}\n  .Because of this, the logit is also called the log-odds since it is equal to the logarithm of the odds \n  \n    \n      \n        \n          \n            p\n            \n              1\n              −\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {p}{1-p}}}\n   where p is a probability. Thus, the logit is a type of function that maps probability values from \n  \n    \n      \n        (\n        0\n        ,\n        1\n        )\n      \n    \n    {\\displaystyle (0,1)}\n   to real numbers in \n  \n    \n      \n        (\n        −\n        ∞\n        ,\n        +\n        ∞\n        )\n      \n    \n    {\\displaystyle (-\\infty ,+\\infty )}\n  , akin to the probit function."})
  ("Odds", {"url": "https://en.wikipedia.org/wiki/Odds", "summary": "In probability theory, odds provide a measure of the likelihood of a particular outcome. They are calculated as the ratio of the number of events that produce that outcome to the number that do not. Odds are commonly used in gambling and statistics. \nOdds also have a simple relation with probability: the odds of an outcome are the ratio of the probability that the outcome occurs to the probability that the outcome does not occur. In mathematical terms, where p is the probability of the outcome: \n\n  \n    \n      \n        \n          odds\n        \n        =\n        \n          \n            p\n            \n              1\n              −\n              p\n            \n          \n        \n      \n    \n    {\\displaystyle {\\text{odds}}={\\frac {p}{1-p}}}\n  where 1 – p is the probability that the outcome does not occur. \nOdds can be demonstrated by examining rolling a six-sided die. The odds of rolling a 6 is 1 to 5 (abbreviated 1:5). This is because there is 1 event (rolling a 6) that produces the specified outcome of "rolling a 6", and 5 events that do not (rolling a 1, 2, 3, 4 or 5). The odds of rolling either a 5 or 6 is 2:4. This is because there are 2 events (rolling a 5 or 6) that produce the specified outcome of "rolling either a 5 or 6", and 4 events that do not (rolling a 1, 2, 3 or 4). The odds of not rolling a 5 or 6 is the inverse 4:2. This is because there are 4 events that produce the specified outcome of "not rolling a 5 or 6" (rolling a 1, 2, 3 or 4) and two that do not (rolling a 5 or 6). \nThe probability of an event is different, but related, and can be calculated from the odds, and vice versa. The probability of rolling a 5 or 6 is the fraction of the number of events over total events or 2/(2+4), which is 1/3, 0.33 or 33%.When gambling, odds are often the ratio of winnings to the stake and you also get your wager returned. So wagering 1 at 1:5 pays out 6 (5 + 1). If you make 6 wagers of 1, and win once and lose 5 times, you will be paid 6 and finish square. Wagering 1 at 1:1 (Evens) pays out 2 (1 + 1) and wagering 1 at 1:2 pays out 3 (1 + 2). These examples may be displayed in different forms, explained later:\n\nFractional odds with a slash: 5 (5/1 against), 1/1 (Evens), 1/2 (on) (short priced horse). Fractional odds can also be written with a colon or a hyphen or dash.\nTote boards use decimal or Continental odds (the ratio of total paid out to stake), e.g. 6.0, 2.0, 1.5\nIn the US Moneyline a positive number lists winnings per $100 wager; a negative number the amount to wager in order to win $100 on a short-priced horse: 500, 100/–100, –200."})
  ("Decimal", {"url": "https://en.wikipedia.org/wiki/Decimal", "summary": "The decimal numeral system (also called the base-ten positional numeral system and denary  or decanary) is the standard system for denoting integer and non-integer numbers. It is the extension to non-integer numbers of the Hindu–Arabic numeral system. The way of denoting numbers in the decimal system is often referred to as decimal notation.A decimal numeral (also often just decimal or, less correctly, decimal number), refers generally to the notation of a number in the decimal numeral system. Decimals may sometimes be identified by a decimal separator (usually "." or "," as in 25.9703 or 3,1415). Decimal may also refer specifically to the digits after the decimal separator, such as in "3.14 is the approximation of π to two decimals". Zero-digits after a decimal separator serve the purpose of signifying the precision of a value.\nThe numbers that may be represented in the decimal system are the decimal fractions. That is, fractions of the form a/10n, where a is an integer, and n is a non-negative integer.\nThe decimal system has been extended to infinite decimals for representing any real number, by using an infinite sequence of digits after the decimal separator (see decimal representation). In this context, the decimal numerals with a finite number of non-zero digits after the decimal separator are sometimes called terminating decimals. A repeating decimal is an infinite decimal that, after some place, repeats indefinitely the same sequence of digits (e.g., 5.123144144144144... = 5.123144). An infinite decimal represents a rational number, the quotient of two integers, if and only if it is a repeating decimal or has a finite number of non-zero digits.\n\n"})
  ("Natural-language user interface", {"url": "https://en.wikipedia.org/wiki/Natural-language_user_interface", "summary": "Natural-language user interface (LUI or NLUI) is a type of computer human interface where linguistic phenomena such as verbs, phrases and clauses act as UI controls for creating, selecting and modifying data in software applications.\nIn interface design, natural-language interfaces are sought after for their speed and ease of use, but most suffer the challenges to understanding wide varieties of ambiguous input.\nNatural-language interfaces are an active area of study in the field of natural-language processing and computational linguistics. An intuitive general natural-language interface is one of the active goals of the Semantic Web.\nText interfaces are "natural" to varying degrees. Many formal (un-natural) programming languages incorporate idioms of natural human language. Likewise, a traditional keyword search engine could be described as a "shallow" natural-language user interface."})
  ("Information science", {"url": "https://en.wikipedia.org/wiki/Information_science", "summary": "Information science (also known as information studies) is an academic field which is primarily concerned with analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. Practitioners within and outside the field study the  application and the usage of knowledge in organizations in addition to the interaction between people, organizations, and any existing information systems with the aim of creating, replacing, improving, or understanding the information systems.\nHistorically, information science (informatics) is associated with computer science, data science, psychology, technology, library science, healthcare, and intelligence agencies. However, information science also incorporates aspects of diverse fields such as archival science, cognitive science, commerce, law, linguistics, museology, management, mathematics, philosophy, public policy, and social sciences."})
  ("Bradford's law", {"url": "https://en.wikipedia.org/wiki/Bradford%27s_law", "summary": "Bradford\"s law is a pattern first described by Samuel C. Bradford in 1934 that estimates the exponentially diminishing returns of searching for references in  science journals. One formulation is that if journals in a field are sorted by number of articles into three groups, each with about one-third of all articles, then the number of journals in each group will be proportional to 1:n:n². There are a number of related  formulations of the principle.\nIn many disciplines, this pattern is called a Pareto distribution. As a practical example, suppose that a researcher has five core scientific journals for his or her subject. Suppose that in a month there are 12 articles of interest in those journals. Suppose further that in order to find another dozen articles of interest, the researcher would have to go to an additional 10 journals. Then that researcher\"s Bradford multiplier bm is 2 (i.e. 10/5). For each new dozen articles, that researcher will need to look in bm times as many journals. After looking in 5, 10, 20, 40, etc. journals, most researchers quickly realize that there is little point in looking further.\nDifferent researchers have different numbers of core journals, and different Bradford multipliers. But the pattern holds quite well across many subjects, and may well be a general pattern for human interactions in social systems. Like Zipf\"s law, to which it is related, we do not have a good explanation for why it works, but knowing that it does is very useful for librarians. What it means is that for each specialty, it is sufficient to identify the "core publications" for that field and only stock those; very rarely will researchers need to go outside that set.\nHowever, its impact has been far greater than that. Armed with this idea and inspired by Vannevar Bush\"s famous article As We May Think, Eugene Garfield at the Institute for Scientific Information in the 1960s developed a comprehensive index of how scientific thinking propagates. His Science Citation Index (SCI) had the effect of making it easy to identify exactly which scientists did science that had an impact, and which journals that science appeared in. It also caused the  discovery, which some did not expect, that a few journals, such as Nature and Science, were core for all of hard science. The same pattern does not happen with the humanities or the social sciences.\nThe result of this is pressure on scientists to publish in the best journals, and pressure on universities to ensure access to that core set of journals. On the other hand, the set of "core journals" may vary more or less strongly with the individual researchers, and even more strongly along schools-of-thought divides. There is also a danger of over-representing majority views if journals are selected in this fashion.\nBradford\"s law is also known as Bradford\"s law of scattering or the Bradford distribution, as it describes how the articles on a particular subject are scattered throughout the mass of periodicals. Another more general term that has come into use since 2006 is information scattering, an often observed phenomenon related to information collections where there are a few sources that have many items of relevant information about a topic, while most sources have only a few. This law of distribution in bibliometrics can be applied to the World Wide Web as well.\n\n"})
  ("Bradford", {"url": "https://en.wikipedia.org/wiki/Bradford", "summary": "Bradford is a city in West Yorkshire, England. It is governed by a metropolitan borough named after the city, the wider county has devolved powers. It had a population of 349,561 at the 2011 census; the second-largest subdivision of the West Yorkshire Built-up Area after Leeds, which is approximately 9 miles (14 km) to the east. The borough had a population of 546,412, making it the 7th most populous district in England.\nHistorically part of the West Riding of Yorkshire, the city grew in the 19th century as an international centre of textile manufacture, particularly wool. It was a boomtown of the Industrial Revolution, and amongst the earliest industrialised settlements, rapidly becoming the "wool capital of the world"; this in turn gave rise to the nicknames "Woolopolis" and "Wool City". Lying in the eastern foothills of the Pennines, the area\"s access to supplies of coal, iron ore and soft water facilitated the growth of a manufacturing base, which, as textile manufacture grew, led to an explosion in population and was a stimulus to civic investment. There is a large amount of listed Victorian architecture in the city including the grand Italianate city hall. It became a municipal borough in 1847, received a city charter in 1897 and, since a 1974 reform, the city limits have been within the current wider borough.\nFrom the mid-20th century, deindustrialisation caused the city\"s textile sector and industrial base to decline and, since then, it has faced similar economic and social challenges to the rest of post-industrial Northern England, including poverty, unemployment and social unrest. It is the third-largest economy within the Yorkshire and the Humber region at around £10 billion, which is mostly provided by financial and manufacturing industries. It is also a tourist destination, the first UNESCO City of Film and it has the National Science and Media Museum, a city park, the Alhambra theatre and Cartwright Hall. The city is the UK City of Culture for 2025 having won the designation on 31 May 2022."})
  ("Information scientist", {"url": "https://en.wikipedia.org/wiki/Information_scientist", "summary": "The term information scientist developed in the latter part of the twentieth century to describe an individual, usually with a relevant subject degree (such as one in Information and Computer Science - CIS) or high level of subject knowledge, providing focused information to scientific and technical research staff in industry. It is a role quite distinct from and complementary to that of a librarian. Developments in end-user searching, together with some convergence between the roles of librarian and information scientist, have led to a diminution in its use in this context, and the term information officer or information professional (information specialist) are also now used.\nThe term was, and is, also used for an individual carrying out research in information science.\nBrian C. Vickery mentions that the Institute of Information Scientists (IIS) was established in London during 1958 and lists the criteria put forward by this institute "Criteria for Information Science" (appendix 1) as well as his own "Areas of study in information science" (appendix 2). The IIS merged with the Library Association in 2002 to form the Chartered Institute of Library and Information Professionals (CILIP)."})
  ("Human–computer information retrieval", {"url": "https://en.wikipedia.org/wiki/Human%E2%80%93computer_information_retrieval", "summary": "Human–computer information retrieval (HCIR) is the study and engineering of information retrieval techniques that bring human intelligence into the search process. It combines the fields of human-computer interaction (HCI) and information retrieval (IR) and creates systems that improve search by taking into account the human context, or through a multi-step search process that provides the opportunity for human feedback."})
  ("Human–computer interaction", {"url": "https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction", "summary": "Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways. A device that allows interaction between human being and a computer is known as a "Human-computer Interface (HCI)".\nAs a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human–Computer Interaction. The first known use was in 1975 by Carlisle. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human–computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field."})
  ("Learning", {"url": "https://en.wikipedia.org/wiki/Learning", "summary": "Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be "lost" from that which cannot be retrieved.Human learning starts at birth (it might even start before in terms of an embryo\"s need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event cannot be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children\"s development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and often associated with representational systems/activity."})
  ("Regression analysis", {"url": "https://en.wikipedia.org/wiki/Regression_analysis", "summary": "In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the "outcome" or "response" variable, or a "label" in machine learning parlance) and one or more independent variables (often called "predictors", "covariates", "explanatory variables" or "features"). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).\nRegression analysis is primarily used for two conceptually distinct purposes.\nFirst, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\nSecond, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data."})
  ("Logistic regression", {"url": "https://en.wikipedia.org/wiki/Logistic_regression", "summary": "In statistics, the logistic model (or logit model) is a statistical model that models the probability of an event taking place by having the log-odds for the event be a linear combination of one or more independent variables. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by an indicator variable, where the two values are labeled "0" and "1", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled "1" can vary between 0 (certainly the value "0") and 1 (certainly the value "1"), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.\nBinary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.\nAnalogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the "simplest" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy.\nThe parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined "logit"; see § History."})
  ("Joint probability distribution", {"url": "https://en.wikipedia.org/wiki/Joint_probability_distribution", "summary": "Given two random variables that are defined on the same probability space, the joint probability distribution is the corresponding probability distribution on all possible pairs of outputs. The joint distribution can just as well be considered for any given number of random variables. The joint distribution encodes the marginal distributions, i.e. the distributions of each of the individual random variables. It also encodes the conditional probability distributions, which deal with how the outputs of one random variable are distributed when given information on the outputs of the other random variable(s).\nIn the formal mathematical setup of measure theory, the joint distribution is given by the pushforward measure, by the map obtained by pairing together the given random variables, of the sample space"s probability measure.\nIn the case of real-valued random variables, the joint distribution, as a particular multivariate distribution, may be expressed by a multivariate cumulative distribution function, or by a multivariate probability density function together with a multivariate probability mass function. In the special case of continuous random variables, it is sufficient to consider probability density functions, and in the case of discrete random variables, it is sufficient to consider probability mass functions."})
  ("Anthropomorphism", {"url": "https://en.wikipedia.org/wiki/Anthropomorphism", "summary": "Anthropomorphism is the attribution of human traits, emotions, or intentions to non-human entities. It is considered to be an innate tendency of human psychology.Personification is the related attribution of human form and characteristics to abstract concepts such as nations, emotions, and natural forces, such as seasons and weather.\nBoth have ancient roots as storytelling and artistic devices, and most cultures have traditional fables with anthropomorphized animals as characters. People have also routinely attributed human emotions and behavioral traits to wild as well as domesticated animals."})
  ("Algorithm", {"url": "https://en.wikipedia.org/wiki/Algorithm", "summary": "In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. More advanced algorithms can use conditionals to divert the code execution through various routes (referred to as automated decision-making) and deduce valid inferences (referred to as automated reasoning), achieving automation eventually. Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as "memory", "search" and "stimulus".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing "output" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input."})
  ("Improvement", {"url": "https://en.wikipedia.org/wiki/Improvement", "summary": "Improvement is the process of a thing moving from one state to a state considered to be better, usually through some action intended to bring about that better state. The concept of improvement is important to governments and businesses, as well as to individuals."})
  ("Feedback", {"url": "https://en.wikipedia.org/wiki/Feedback", "summary": "Feedback occurs when outputs of a system are routed back as inputs as part of a chain of cause-and-effect that forms a circuit or loop. The system can then be said to feed back into itself. The notion of cause-and-effect has to be handled carefully when applied to feedback systems:\n\nSimple causal reasoning about a feedback system is difficult because the first system influences the second and second system influences the first, leading to a circular argument. This makes reasoning based upon cause and effect tricky, and it is necessary to analyze the system as a whole. As provided by Webster, feedback in business is the transmission of evaluative or corrective information about an action, event, or process to the original or controlling source."})
  ("Length", {"url": "https://en.wikipedia.org/wiki/Length", "summary": "Length is a measure of distance. In the International System of Quantities, length is a quantity with dimension distance. In most systems of measurement a base unit for length is chosen, from which all other units are derived. In the International System of Units (SI) system the base unit for length is the metre.\nLength is commonly understood to mean the most extended dimension of a fixed object. However, this is not always the case and may depend on the position the object is in.\nVarious terms for the length of a fixed object are used, and these include height, which is vertical length or vertical extent, and width, breadth or depth. Height is used when there is a base from which vertical measurements can be taken. Width or breadth usually refer to a shorter dimension when length is the longest one. Depth is used for the third dimension of a three dimensional object.Length is the measure of one spatial dimension, whereas area is a measure of two dimensions (length squared) and volume is a measure of three dimensions (length cubed)."})
  ("Ranking", {"url": "https://en.wikipedia.org/wiki/Ranking", "summary": "A ranking is a relationship between a set of items such that, for any two items, the first is either "ranked higher than", "ranked lower than" or "ranked equal to" the second.\nIn mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.\nBy reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see.\nAnalysis of data obtained by ranking commonly requires non-parametric statistics."})
  ("Adl", {"url": "https://en.wikipedia.org/wiki/Adl", "summary": "Adl (Arabic: عدل, ʻadl) is an Arabic word meaning "justice", and is also one of the names of God in Islam. It is equal to the concept of Insaf انصاف (lit. sense of justice) in the Baháʼí Faith.Adil (Arabic: عادل, ʻādil), and Adeel (Arabic: عديل, ʻadīl) are male names derived from ʻadl and are common throughout the Muslim world."})
  ("Apache Lucene", {"url": "https://en.wikipedia.org/wiki/Apache_Lucene", "summary": "Apache Lucene is a free and open-source search engine software library, originally written in Java by Doug Cutting. It is supported by the Apache Software Foundation and is released under the Apache Software License.  Lucene is widely used as a standard foundation for non-research search applications.Lucene has been ported to other programming languages including Object Pascal, Perl, C#, C++, Python, Ruby and PHP."})
  ("SpaCy", {"url": "https://en.wikipedia.org/wiki/SpaCy", "summary": "spaCy ( spay-SEE) is an open-source software library for advanced natural language processing, written in the programming languages Python and Cython. The library is published under the MIT license and its main developers are Matthew Honnibal and Ines Montani, the founders of the software company Explosion.\nUnlike NLTK, which is widely used for teaching and research, spaCy focuses on providing software for production usage. spaCy also supports deep learning workflows that allow connecting statistical models trained by popular machine learning libraries like TensorFlow, PyTorch or MXNet through its own machine learning library Thinc. Using Thinc as its backend, spaCy features convolutional neural network models for part-of-speech tagging, dependency parsing, text categorization and named entity recognition (NER). Prebuilt statistical neural network models to perform these tasks are available for 23 languages, including English, Portuguese, Spanish, Russian and Chinese, and there is also a multi-language NER model. Additional support for tokenization for more than 65 languages allows users to train custom models on their own datasets as well."})
  ("Library", {"url": "https://en.wikipedia.org/wiki/Library", "summary": "A library is a collection of materials, books or media that are accessible for use and not just for display purposes. A library provides physical (hard copies) or digital access (soft copies) materials, and may be a physical location or a virtual space, or both. A library\"s collection can include printed materials and other physical resources in many formats such as DVD, CD and cassette as well as access to information, music or other content held on bibliographic databases.\nA library, which may vary widely in size, may be organized for use and maintained by a public body such as a government; an institution such as a school or museum; a corporation; or a private individual. In addition to providing materials, libraries also provide the services of librarians who are trained and experts at finding, selecting, circulating and organizing information and at interpreting information needs, navigating and analyzing very large amounts of information with a variety of resources.\nLibrary buildings often provide quiet areas for studying, as well as common areas for group study and collaboration, and may provide public facilities for access to their electronic resources; for instance: computers and access to the Internet. The library\"s clientele and services offered vary depending on its type: users of a public library have different needs from those of a special library or academic library, for example. Libraries may also be community hubs, where programs are delivered and people engage in lifelong learning. Modern libraries extend their services beyond the physical walls of a building by providing material accessible by electronic means, including from home via the Internet.\nThe services that libraries offer are variously described as library services, information services, or the combination "library and information services", although different institutions and sources define such terminology differently."})
  ("Transformers", {"url": "https://en.wikipedia.org/wiki/Transformers", "summary": "Transformers is a media franchise produced by American toy company Hasbro and Japanese toy company Takara Tomy. It primarily follows the heroic Autobots and the villainous Decepticons, two alien robot factions at war that can transform into other forms, such as vehicles and animals. The franchise encompasses toys, animation, comic books, video games and films. As of 2011, it generated more than ¥2 trillion ($25 billion) in revenue, making it one of the highest-grossing media franchises of all time.\nThe franchise began in 1984 with the Transformers toy line, comprising transforming mecha toys from Takara\"s Diaclone and Micro Change toylines rebranded for Western markets. The term "Generation 1" covers both the animated television series The Transformers and the comic book series of the same name, which are further divided into Japanese, British and Canadian spin-offs, respectively. Sequels followed, such as the Generation 2 comic book and Beast Wars TV series, which became its own mini-universe. Generation 1 characters underwent two reboots with Dreamwave Productions in 2001 and IDW Publishing in 2005, with a third starting in 2019. There have been other incarnations of the story based on different toy lines during and after the 20th century. The first was the Robots in Disguise series, followed by three shows (Armada, Energon, and Cybertron) that constitute a single universe called the "Unicron Trilogy".\nA live-action film series started in 2007, again distinct from previous incarnations, while the Transformers: Animated series merged concepts from the G1 continuity, the 2007 live-action film and the "Unicron Trilogy". For most of the 2010s, in an attempt to mitigate the wave of reboots, the "Aligned Continuity" was established. In 2018, Transformers: Cyberverse debuted, once again, distinct from the previous incarnations.\nAlthough initially a separate and competing franchise started in 1983, Tonka\"s GoBots became the intellectual property of Hasbro after their buyout of Tonka in 1991. Subsequently, the universe depicted in the animated series Challenge of the GoBots and follow-up film GoBots: Battle of the Rock Lords was retroactively established as an alternate universe within the Transformers multiverse."})
  ("Data set", {"url": "https://en.wikipedia.org/wiki/Data_set", "summary": "A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European data.europa.eu portal aggregates more than a million data sets.  Some other issues (real-time data sources, non-relational data sets, etc.) increases the difficulty to reach a consensus about it.\n\n"})
  ("Reading", {"url": "https://en.wikipedia.org/wiki/Reading", "summary": "Reading is the process of taking in the sense or meaning of letters, symbols, etc., especially by sight or touch.For educators and researchers, reading is a multifaceted process involving such areas as word recognition, orthography (spelling), alphabetics, phonics, phonemic awareness, vocabulary, comprehension, fluency, and motivation.Other types of reading and writing, such as pictograms (e.g., a hazard symbol and an emoji), are not based on speech-based writing systems. The common link is the interpretation of symbols to extract the meaning from the visual notations or tactile signals (as in the case of braille)."})
